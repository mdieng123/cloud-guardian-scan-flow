PROVIDER;DESCRIPTION;PROJECTID;LOCATION;ASSESSMENTDATE;REQUIREMENTS_ID;REQUIREMENTS_DESCRIPTION;REQUIREMENTS_ATTRIBUTES_SECTION;REQUIREMENTS_ATTRIBUTES_SUBSECTION;REQUIREMENTS_ATTRIBUTES_PROFILE;REQUIREMENTS_ATTRIBUTES_ASSESSMENTSTATUS;REQUIREMENTS_ATTRIBUTES_DESCRIPTION;REQUIREMENTS_ATTRIBUTES_RATIONALESTATEMENT;REQUIREMENTS_ATTRIBUTES_IMPACTSTATEMENT;REQUIREMENTS_ATTRIBUTES_REMEDIATIONPROCEDURE;REQUIREMENTS_ATTRIBUTES_AUDITPROCEDURE;REQUIREMENTS_ATTRIBUTES_ADDITIONALINFORMATION;REQUIREMENTS_ATTRIBUTES_REFERENCES;STATUS;STATUSEXTENDED;RESOURCEID;RESOURCENAME;CHECKID;MUTED
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket export-2emvipe22ql72utfqfya is not publicly accessible.;export-2emvipe22ql72utfqfya;export-2emvipe22ql72utfqfya;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket export-7ea6nbf3rfxh7lc7hmdq is not publicly accessible.;export-7ea6nbf3rfxh7lc7hmdq;export-7ea6nbf3rfxh7lc7hmdq;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket export-ag6yvqffmoie2rpocyha is not publicly accessible.;export-ag6yvqffmoie2rpocyha;export-ag6yvqffmoie2rpocyha;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket export-czkghfu6w6pav2frsxyq is not publicly accessible.;export-czkghfu6w6pav2frsxyq;export-czkghfu6w6pav2frsxyq;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-CENTRAL1;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket run-sources-inbound-entity-461511-j4-us-central1 is not publicly accessible.;run-sources-inbound-entity-461511-j4-us-central1;run-sources-inbound-entity-461511-j4-us-central1;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-EAST1;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket run-sources-inbound-entity-461511-j4-us-east1 is not publicly accessible.;run-sources-inbound-entity-461511-j4-us-east1;run-sources-inbound-entity-461511-j4-us-east1;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-EAST4;2025-07-01 12:41:26.937059;5.1;Ensure That Cloud Storage Bucket Is Not Anonymously or Publicly Accessible;5 Storage;;Level 1;Automated;It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous or public access.;Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data. Hence, ensure that anonymous or public access to a bucket is not allowed.;No storage buckets would be publicly accessible. You would have to explicitly administer bucket access.;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on the bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab. 4. Click `Delete` button in front of `allUsers` and `allAuthenticatedUsers` to remove that particular role assignment.**From Google Cloud CLI**Remove `allUsers` and `allAuthenticatedUsers` access.```gsutil iam ch -d allUsers gs://BUCKET_NAMEgsutil iam ch -d allAuthenticatedUsers gs://BUCKET_NAME```**Prevention:**You can prevent Storage buckets from becoming publicly accessible by setting up the `Domain restricted sharing` organization policy at:[ https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains ](https://console.cloud.google.com/iam-admin/orgpolicies/iam-allowedPolicyMemberDomains).;**From Google Cloud Console**1. Go to `Storage browser` by visiting [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser).2. Click on each bucket name to go to its `Bucket details` page.3. Click on the `Permissions` tab.4. Ensure that `allUsers` and `allAuthenticatedUsers` are not in the `Members` list.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. Check the IAM Policy for each bucket:```gsutil iam get gs://BUCKET_NAME```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.**Using Rest API**1. List all buckets in a project```Get https://www.googleapis.com/storage/v1/b?project=<ProjectName>```2. Check the IAM Policy for each bucket```GET https://www.googleapis.com/storage/v1/b/<bucketName>/iam```No role should contain `allUsers` and/or `allAuthenticatedUsers` as a member.;"To implement Access restrictions on buckets, configuring Bucket IAM is preferred way than configuring Bucket ACL. On GCP console, ""Edit Permissions"" for bucket exposes IAM configurations only. Bucket ACLs are configured automatically as per need in order to implement/support User enforced Bucket IAM policy. In-case administrator changes bucket ACL using command-line(gsutils)/API bucket IAM also gets updated automatically.";https://cloud.google.com/storage/docs/access-control/iam-reference:https://cloud.google.com/storage/docs/access-control/making-data-public:https://cloud.google.com/storage/docs/gsutil/commands/iam;PASS;Bucket run-sources-inbound-entity-461511-j4-us-east4 is not publicly accessible.;run-sources-inbound-entity-461511-j4-us-east4;run-sources-inbound-entity-461511-j4-us-east4;cloudstorage_bucket_public_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;FAIL;Bucket export-2emvipe22ql72utfqfya has uniform Bucket Level Access disabled.;export-2emvipe22ql72utfqfya;export-2emvipe22ql72utfqfya;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;FAIL;Bucket export-7ea6nbf3rfxh7lc7hmdq has uniform Bucket Level Access disabled.;export-7ea6nbf3rfxh7lc7hmdq;export-7ea6nbf3rfxh7lc7hmdq;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;FAIL;Bucket export-ag6yvqffmoie2rpocyha has uniform Bucket Level Access disabled.;export-ag6yvqffmoie2rpocyha;export-ag6yvqffmoie2rpocyha;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;FAIL;Bucket export-czkghfu6w6pav2frsxyq has uniform Bucket Level Access disabled.;export-czkghfu6w6pav2frsxyq;export-czkghfu6w6pav2frsxyq;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-CENTRAL1;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;PASS;Bucket run-sources-inbound-entity-461511-j4-us-central1 has uniform Bucket Level Access enabled.;run-sources-inbound-entity-461511-j4-us-central1;run-sources-inbound-entity-461511-j4-us-central1;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-EAST1;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;PASS;Bucket run-sources-inbound-entity-461511-j4-us-east1 has uniform Bucket Level Access enabled.;run-sources-inbound-entity-461511-j4-us-east1;run-sources-inbound-entity-461511-j4-us-east1;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;US-EAST4;2025-07-01 12:41:26.937059;5.2;Ensure That Cloud Storage Buckets Have Uniform Bucket-Level Access Enabled;5 Storage;;Level 2;Automated;It is recommended that uniform bucket-level access is enabled on Cloud Storage buckets.;It is recommended to use uniform bucket-level access to unify and simplify how you grant access to your Cloud Storage resources. Cloud Storage offers two systems for granting users permission to access your buckets and objects: Cloud Identity and Access Management (Cloud IAM) and Access Control Lists (ACLs). These systems act in parallel - in order for a user to access a Cloud Storage resource, only one of the systems needs to grant the user permission. Cloud IAM is used throughout Google Cloud and allows you to grant a variety of permissions at the bucket and project levels. ACLs are used only by Cloud Storage and have limited permission options, but they allow you to grant permissions on a per-object basis.In order to support a uniform permissioning system, Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud Storage resources: access to Cloud Storage resources then is granted exclusively through Cloud IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.;If you enable uniform bucket-level access, you revoke access from users who gain their access solely through object ACLs.Certain Google Cloud services, such as Stackdriver, Cloud Audit Logs, and Datastore, cannot export to Cloud Storage buckets that have uniform bucket-level access enabled.;**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. In the list of buckets, click on the name of the desired bucket.3. Select the `Permissions` tab near the top of the page.4. In the text box that starts with `This bucket uses fine-grained access control...`, click `Edit`.5. In the pop-up menu that appears, select `Uniform`.6. Click `Save`.**From Google Cloud CLI**Use the on option in a uniformbucketlevelaccess set command:```gsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/```**Prevention**You can set up an Organization Policy to enforce that any new bucket has uniform bucket level access enabled. Learn more at:[https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket](https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket);**From Google Cloud Console**1. Open the Cloud Storage browser in the Google Cloud Console by visiting: [https://console.cloud.google.com/storage/browser](https://console.cloud.google.com/storage/browser)2. For each bucket, make sure that `Access control` column has the value `Uniform`.**From Google Cloud CLI**1. List all buckets in a project```gsutil ls```2. For each bucket, verify that uniform bucket-level access is enabled.```gsutil uniformbucketlevelaccess get gs://BUCKET_NAME/```If uniform bucket-level access is enabled, the response looks like:```Uniform bucket-level access setting for gs://BUCKET_NAME/: Enabled: True LockedTime: LOCK_DATE```;Uniform bucket-level access can no longer be disabled if it has been active on a bucket for 90 consecutive days.;https://cloud.google.com/storage/docs/uniform-bucket-level-access:https://cloud.google.com/storage/docs/using-uniform-bucket-level-access:https://cloud.google.com/storage/docs/setting-org-policies#uniform-bucket;PASS;Bucket run-sources-inbound-entity-461511-j4-us-east4 has uniform Bucket Level Access enabled.;run-sources-inbound-entity-461511-j4-us-east4;run-sources-inbound-entity-461511-j4-us-east4;cloudstorage_bucket_uniform_bucket_level_access;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.7;Ensure That RDP Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow users to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the Internet to a VPC or VM instance using `RDP` on `Port 3389` can be avoided.;GCP `Firewall Rules` within a `VPC Network`. These rules apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `RDP` with the default `Port 3389`. Generic access from the Internet to a specific IP Range should be restricted.;All Remote Desktop Protocol (RDP) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where secure shell access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to RDP port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` to be modified.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update RDP Firewall rule with new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure `Port` is not equal to `3389` and `Action` is not `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed.ports)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `TCP` or `ALL`- AND `PORTS` is set to `3389` or `range containing 3389` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-icmp does not expose port 3389 (RDP) to the internet.;4831585100056248501;default-allow-icmp;compute_firewall_rdp_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.7;Ensure That RDP Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow users to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the Internet to a VPC or VM instance using `RDP` on `Port 3389` can be avoided.;GCP `Firewall Rules` within a `VPC Network`. These rules apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `RDP` with the default `Port 3389`. Generic access from the Internet to a specific IP Range should be restricted.;All Remote Desktop Protocol (RDP) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where secure shell access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to RDP port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` to be modified.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update RDP Firewall rule with new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure `Port` is not equal to `3389` and `Action` is not `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed.ports)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `TCP` or `ALL`- AND `PORTS` is set to `3389` or `range containing 3389` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-internal does not expose port 3389 (RDP) to the internet.;7829697015417946293;default-allow-internal;compute_firewall_rdp_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.7;Ensure That RDP Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow users to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the Internet to a VPC or VM instance using `RDP` on `Port 3389` can be avoided.;GCP `Firewall Rules` within a `VPC Network`. These rules apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `RDP` with the default `Port 3389`. Generic access from the Internet to a specific IP Range should be restricted.;All Remote Desktop Protocol (RDP) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where secure shell access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to RDP port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` to be modified.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update RDP Firewall rule with new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure `Port` is not equal to `3389` and `Action` is not `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed.ports)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `TCP` or `ALL`- AND `PORTS` is set to `3389` or `range containing 3389` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;FAIL;Firewall default-allow-rdp does exposes port 3389 (RDP) to the internet.;4037445621256044725;default-allow-rdp;compute_firewall_rdp_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.7;Ensure That RDP Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow users to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the Internet to a VPC or VM instance using `RDP` on `Port 3389` can be avoided.;GCP `Firewall Rules` within a `VPC Network`. These rules apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `RDP` with the default `Port 3389`. Generic access from the Internet to a specific IP Range should be restricted.;All Remote Desktop Protocol (RDP) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where secure shell access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to RDP port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` to be modified.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update RDP Firewall rule with new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure `Port` is not equal to `3389` and `Action` is not `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed.ports)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `TCP` or `ALL`- AND `PORTS` is set to `3389` or `range containing 3389` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-ssh does not expose port 3389 (RDP) to the internet.;4381256950141960373;default-allow-ssh;compute_firewall_rdp_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.6;Ensure That SSH Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow the user to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, only an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the internet to VPC or VM instance using `SSH` on `Port 22` can be avoided.;GCP `Firewall Rules` within a `VPC Network` apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `SSH` with the default `Port 22`. Generic access from the Internet to a specific IP Range needs to be restricted.;All Secure Shell (SSH) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where SSH access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to SSH port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` you want to modify.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update the Firewall rule with the new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure that `Port` is not equal to `22` and `Action` is not set to `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `tcp` or `ALL`- AND `PORTS` is set to `22` or `range containing 22` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-icmp does not expose port 22 (SSH) to the internet.;4831585100056248501;default-allow-icmp;compute_firewall_ssh_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.6;Ensure That SSH Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow the user to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, only an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the internet to VPC or VM instance using `SSH` on `Port 22` can be avoided.;GCP `Firewall Rules` within a `VPC Network` apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `SSH` with the default `Port 22`. Generic access from the Internet to a specific IP Range needs to be restricted.;All Secure Shell (SSH) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where SSH access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to SSH port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` you want to modify.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update the Firewall rule with the new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure that `Port` is not equal to `22` and `Action` is not set to `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `tcp` or `ALL`- AND `PORTS` is set to `22` or `range containing 22` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-internal does not expose port 22 (SSH) to the internet.;7829697015417946293;default-allow-internal;compute_firewall_ssh_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.6;Ensure That SSH Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow the user to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, only an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the internet to VPC or VM instance using `SSH` on `Port 22` can be avoided.;GCP `Firewall Rules` within a `VPC Network` apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `SSH` with the default `Port 22`. Generic access from the Internet to a specific IP Range needs to be restricted.;All Secure Shell (SSH) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where SSH access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to SSH port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` you want to modify.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update the Firewall rule with the new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure that `Port` is not equal to `22` and `Action` is not set to `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `tcp` or `ALL`- AND `PORTS` is set to `22` or `range containing 22` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;PASS;Firewall default-allow-rdp does not expose port 22 (SSH) to the internet.;4037445621256044725;default-allow-rdp;compute_firewall_ssh_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.6;Ensure That SSH Access Is Restricted From the Internet;3 Networking;;Level 2;Automated;GCP `Firewall Rules` are specific to a `VPC Network`. Each rule either `allows` or `denies` traffic when its conditions are met. Its conditions allow the user to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.Firewall rules are defined at the VPC network level and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, only an `IPv4` address or `IPv4 block in CIDR` notation can be used. Generic `(0.0.0.0/0)` incoming traffic from the internet to VPC or VM instance using `SSH` on `Port 22` can be avoided.;GCP `Firewall Rules` within a `VPC Network` apply to outgoing (egress) traffic from instances and incoming (ingress) traffic to instances in the network. Egress and ingress traffic flows are controlled even if the traffic stays within the network (for example, instance-to-instance communication).For an instance to have outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is specified. This route simply defines the path to the Internet, to avoid the most general `(0.0.0.0/0)` destination `IP Range` specified from the Internet through `SSH` with the default `Port 22`. Generic access from the Internet to a specific IP Range needs to be restricted.;All Secure Shell (SSH) connections from outside of the network to the concerned VPC(s) will be blocked. There could be a business need where SSH access is required from outside of the network to access resources associated with the VPC. In that case, specific source IP(s) should be mentioned in firewall rules to white-list access to SSH port for the concerned VPC(s).;**From Google Cloud Console**1. Go to `VPC Network`.2. Go to the `Firewall Rules`.3. Click the `Firewall Rule` you want to modify.4. Click `Edit`.5. Modify `Source IP ranges` to specific `IP`.6. Click `Save`.**From Google Cloud CLI**1.Update the Firewall rule with the new `SOURCE_RANGE` from the below command: gcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[:PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...];**From Google Cloud Console**1. Go to `VPC network`.2. Go to the `Firewall Rules`.3. Ensure that `Port` is not equal to `22` and `Action` is not set to `Allow`.4. Ensure `IP Ranges` is not equal to `0.0.0.0/0` under `Source filters`.**From Google Cloud CLI** gcloud compute firewall-rules list --format=table'(name,direction,sourceRanges,allowed)'Ensure that there is no rule matching the below criteria:- `SOURCE_RANGES` is `0.0.0.0/0`- AND `DIRECTION` is `INGRESS`- AND IPProtocol is `tcp` or `ALL`- AND `PORTS` is set to `22` or `range containing 22` or `Null (not set)`Note: - When ALL TCP ports are allowed in a rule, PORT does not have any value set (`NULL`)- When ALL Protocols are allowed in a rule, PORT does not have any value set (`NULL`);"Currently, GCP VPC only supports IPV4; however, Google is already working on adding IPV6 support for VPC. In that case along with source IP range `0.0.0.0`, the rule should be checked for IPv6 equivalent `::/0` as well.";https://cloud.google.com/vpc/docs/firewalls#blockedtraffic:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;FAIL;Firewall default-allow-ssh does exposes port 22 (SSH) to the internet.;4381256950141960373;default-allow-ssh;compute_firewall_ssh_access_from_the_internet_allowed;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.1;Ensure That the Default Network Does Not Exist in a Project;3 Networking;;Level 2;Automated;To prevent use of `default` network, a project should not have a `default` network.;The `default` network has a preconfigured network configuration and automatically generates the following insecure firewall rules: - default-allow-internal: Allows ingress connections for all protocols and ports among instances in the network.- default-allow-ssh: Allows ingress connections on TCP port 22(SSH) from any source to any instance in the network.- default-allow-rdp: Allows ingress connections on TCP port 3389(RDP) from any source to any instance in the network.- default-allow-icmp: Allows ingress ICMP traffic from any source to any instance in the network.These automatically created firewall rules do not get audit logged by default. Furthermore, the default network is an auto mode network, which means that its subnets use the same predefined range of IP addresses, and as a result, it's not possible to use Cloud VPN or VPC Network Peering with the default network. Based on organization security and networking requirements, the organization should create a new network and delete the `default` network.;When an organization deletes the default network, it will need to remove all asests from that network and migrate them to a new network.;**From Google Cloud Console**1. Go to the `VPC networks` page by visiting: [https://console.cloud.google.com/networking/networks/list](https://console.cloud.google.com/networking/networks/list).2. Click the network named `default`.2. On the network detail page, click `EDIT`.3. Click `DELETE VPC NETWORK`.4. If needed, create a new network to replace the default network.**From Google Cloud CLI**For each Google Cloud Platform project,1. Delete the default network:```gcloud compute networks delete default```2. If needed, create a new network to replace it:```gcloud compute networks create NETWORK_NAME```**Prevention:**The user can prevent the default network and its insecure default firewall rules from being created by setting up an Organization Policy to `Skip default network creation` at [https://console.cloud.google.com/iam-admin/orgpolicies/compute-skipDefaultNetworkCreation](https://console.cloud.google.com/iam-admin/orgpolicies/compute-skipDefaultNetworkCreation).;**From Google Cloud Console**1. Go to the `VPC networks` page by visiting: [https://console.cloud.google.com/networking/networks/list](https://console.cloud.google.com/networking/networks/list).2. Ensure that a network with the name `default` is not present.**From Google Cloud CLI**1. Set the project name in the Google Cloud Shell:```gcloud config set project PROJECT_ID ```2. List the networks configured in that project:```gcloud compute networks list ```It should not list `default` as one of the available networks in that project.;;https://cloud.google.com/compute/docs/networking#firewall_rules:https://cloud.google.com/compute/docs/reference/latest/networks/insert:https://cloud.google.com/compute/docs/reference/latest/networks/delete:https://cloud.google.com/vpc/docs/firewall-rules-logging:https://cloud.google.com/vpc/docs/vpc#default-network:https://cloud.google.com/sdk/gcloud/reference/compute/networks/delete;FAIL;Default network is in use in project inbound-entity-461511-j4.;default;default;compute_network_default_in_use;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;3.2;Ensure Legacy Networks Do Not Exist for Older Projects;3 Networking;;Level 1;Automated;In order to prevent use of legacy networks, a project should not have a legacy network configured. As of now, Legacy Networks are gradually being phased out, and you can no longer create projects with them. This recommendation is to check older projects to ensure that they are not using Legacy Networks.;Legacy networks have a single network IPv4 prefix range and a single gateway IP address for the whole network. The network is global in scope and spans all cloud regions. Subnetworks cannot be created in a legacy network and are unable to switch from legacy to auto or custom subnet networks. Legacy networks can have an impact for high network traffic projects and are subject to a single point of contention or failure.;None.;**From Google Cloud CLI**For each Google Cloud Platform project,1. Follow the documentation and create a non-legacy network suitable for the organization's requirements.2. Follow the documentation and delete the networks in the `legacy` mode.;**From Google Cloud CLI**For each Google Cloud Platform project,1. Set the project name in the Google Cloud Shell:```gcloud config set project <Project-ID> ```2. List the networks configured in that project:```gcloud compute networks list ```None of the listed networks should be in the `legacy` mode.;;https://cloud.google.com/vpc/docs/using-legacy#creating_a_legacy_network:https://cloud.google.com/vpc/docs/using-legacy#deleting_a_legacy_network;PASS;Network default is not legacy.;4640827988780007637;default;compute_network_not_legacy;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;4.4;Ensure Oslogin Is Enabled for a Project;4 Virtual Machines;;Level 1;Automated;Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management.;Enabling osLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to IAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and automated SSH key pair management which is useful in handling cases like response to compromised SSH key pairs and/or revocation of external/third-party/Vendor users.;Enabling OS Login on project disables metadata-based SSH key configurations on all instances from a project. Disabling OS Login restores SSH keys that you have configured in project or instance meta-data.;**From Google Cloud Console**1. Go to the VM compute metadata page by visiting: [https://console.cloud.google.com/compute/metadata](https://console.cloud.google.com/compute/metadata).2. Click `Edit`.3. Add a metadata entry where the key is `enable-oslogin` and the value is `TRUE`.4. Click `Save` to apply the changes.5. For every instance that overrides the project setting, go to the `VM Instances` page at [https://console.cloud.google.com/compute/instances](https://console.cloud.google.com/compute/instances).6. Click the name of the instance on which you want to remove the metadata value.7. At the top of the instance details page, click `Edit` to edit the instance settings.8. Under `Custom metadata`, remove any entry with key `enable-oslogin` and the value is `FALSE`9. At the bottom of the instance details page, click `Save` to apply your changes to the instance.**From Google Cloud CLI**1. Configure oslogin on the project:```gcloud compute project-info add-metadata --metadata enable-oslogin=TRUE```2. Remove instance metadata that overrides the project setting.```gcloud compute instances remove-metadata <INSTANCE_NAME> --keys=enable-oslogin```Optionally, you can enable two factor authentication for OS login. For more information, see: [https://cloud.google.com/compute/docs/oslogin/setup-two-factor-authentication](https://cloud.google.com/compute/docs/oslogin/setup-two-factor-authentication).;**From Google Cloud Console**1. Go to the VM compute metadata page by visiting [https://console.cloud.google.com/compute/metadata](https://console.cloud.google.com/compute/metadata).2. Ensure that key `enable-oslogin` is present with value set to `TRUE`. 3. Because instances can override project settings, ensure that no instance has custom metadata with key `enable-oslogin` and value `FALSE`.**From Google Cloud CLI**1. List the instances in your project and get details on each instance:```gcloud compute instances list --format=json```2. Verify that the section `commonInstanceMetadata` has a key `enable-oslogin` set to value `TRUE`.**Exception:**VMs created by GKE should be excluded. These VMs have names that start with `gke-` and are labeled `goog-gke-node`;1. In order to use osLogin, instance using Custom Images must have the latest version of the Linux Guest Environment installed. The following image families do not yet support OS Login:```Project cos-cloud (Container-Optimized OS) image family cos-stable.All project coreos-cloud (CoreOS) image familiesProject suse-cloud (SLES) image family sles-11All Windows Server and SQL Server image families```2. Project enable-oslogin can be over-ridden by setting enable-oslogin parameter to an instance metadata individually.;https://cloud.google.com/compute/docs/instances/managing-instance-access:https://cloud.google.com/compute/docs/instances/managing-instance-access#enable_oslogin:https://cloud.google.com/sdk/gcloud/reference/compute/instances/remove-metadata:https://cloud.google.com/compute/docs/oslogin/setup-two-factor-authentication;FAIL;Project inbound-entity-461511-j4 does not have OS Login enabled.;inbound-entity-461511-j4;My First Project;compute_project_os_login_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-east4;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;7501131987209175214;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-east1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;5276167185023910062;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-central1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4066485020425732270;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-east5;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;7915361911958159533;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;northamerica-northeast2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4343465988170989741;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west4;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8131200743184810157;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-southwest1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;33015601356882093;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west3;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8513126466553921709;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west12;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8553495890666209453;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-west3;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6181008947662307501;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;16346871935811757;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west8;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8041309268112851117;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-west4;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6698361178922172589;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-north1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;343017553279074477;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west10;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;9025139933205774509;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;southamerica-west1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8251863116311969965;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west9;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;1404553002914431149;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;northamerica-northeast1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;9164831012908974253;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-central2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4419043647210406061;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-northeast3;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;689524459474534573;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west6;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;5639549077854896301;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;me-west1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4037073496699584685;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-east1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6980668722158256301;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;southamerica-east1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6181888166122504365;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-northeast2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8746008879062335661;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-south1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4500715903497552045;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;northamerica-south1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;7234548533992112301;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-west1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;1723146306767831214;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;us-west2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;5347385456552853677;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-southeast2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;1585873585710055597;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-west2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;520349258212982957;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-east2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;1016280043731304621;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;australia-southeast1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;5007487409809277101;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;australia-southeast2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;3307157733323325613;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;europe-north2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4609668898695692461;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-northeast1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;8995685584360718509;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;me-central1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4178596994134851757;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-southeast1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;3849405519176155309;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;africa-south1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6525477586933137581;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-south1;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;4509980848034693293;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;asia-south2;2025-07-01 12:41:26.937059;3.8;Ensure that VPC Flow Logs is Enabled for Every Subnet in a VPC Network;3 Networking;;Level 2;Automated;Flow Logs is a feature that enables users to capture information about the IP traffic going to and from network interfaces in the organization's VPC Subnets. Once a flow log is created, the user can view and retrieve its data in Stackdriver Logging. It is recommended that Flow Logs be enabled for every business-critical VPC subnet.;VPC networks and subnetworks not reserved for internal HTTP(S) load balancing provide logically isolated and secure network partitions where GCP resources can be launched. When Flow Logs are enabled for a subnet, VMs within that subnet start reporting on all Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) flows.Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from another VM, a host in the on-premises datacenter, a Google service, or a host on the Internet. If two GCP VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.Flow Logs supports the following use cases:- Network monitoring- Understanding network usage and optimizing network traffic expenses- Network forensics- Real-time security analysisFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect anomalous traffic or provide insight during security workflows.The Flow Logs must be configured such that all network traffic is logged, the interval of logging is granular to provide detailed information on the connections, no logs are filtered, and metadata to facilitate investigations are included.**Note**: Subnets reserved for use by internal HTTP(S) load balancers do not support VPC flow logs.;Standard pricing for Stackdriver Logging, BigQuery, or Cloud Pub/Sub applies. VPC Flow Logs generation will be charged starting in GA as described in reference: https://cloud.google.com/vpc/;**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. Click the name of a subnet, The `Subnet details` page displays.3. Click the `EDIT` button.4. Set `Flow Logs` to `On`.5. Expand the `Configure Logs` section.6. Set `Aggregation Interval` to `5 SEC`.7. Check the box beside `Include metadata`.8. Set `Sample rate` to `100`.9. Click Save.**Note**: It is not possible to configure a Log filter from the console.**From Google Cloud CLI**To enable VPC Flow Logs for a network subnet, run the following command:```gcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all```;"**From Google Cloud Console**1. Go to the VPC network GCP Console visiting `https://console.cloud.google.com/networking/networks/list` 2. From the list of network subnets, make sure for each subnet:- `Flow Logs` is set to `On`- `Aggregation Interval` is set to `5 sec`- `Include metadata` checkbox is checked- `Sample rate` is set to `100%`**Note**: It is not possible to determine if a Log filter has been defined from the console.**From Google Cloud CLI**```gcloud compute networks subnets list --format json | \ jq -r '([""Subnet"",""Purpose"",""Flow_Logs"",""Aggregation_Interval"",""Flow_Sampling"",""Metadata"",""Logs_Filtered""] | (., map(length*""-""))),  (.[] |  [ .name,  .purpose, (if has(""enableFlowLogs"") and .enableFlowLogs == true then ""Enabled"" else ""Disabled"" end), (if has(""logConfig"") then .logConfig.aggregationInterval else ""N/A"" end), (if has(""logConfig"") then .logConfig.flowSampling else ""N/A"" end), (if has(""logConfig"") then .logConfig.metadata else ""N/A"" end), (if has(""logConfig"") then (.logConfig | has(""filterExpr"")) else ""N/A"" end) ] ) |  @tsv' | \ column -t```The output of the above command will list:- each subnet- the subnet's purpose- a `Enabled` or `Disabled` value if `Flow Logs` are enabled- the value for `Aggregation Interval` or `N/A` if disabled, the value for `Flow Sampling` or `N/A` if disabled- the value for `Metadata` or `N/A` if disabled- 'true' or 'false' if a Logging Filter is configured or 'N/A' if disabled.If the subnet's purpose is `PRIVATE` then `Flow Logs` should be `Enabled`.If `Flow Logs` is enabled then:- `Aggregation_Interval` should be `INTERVAL_5_SEC`- `Flow_Sampling` should be 1- `Metadata` should be `INCLUDE_ALL_METADATA`- `Logs_Filtered` should be `false`.";;https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging:https://cloud.google.com/vpc/;FAIL;Subnet default in network default does not have flow logs enabled.;6044575759566913709;default;compute_subnet_flow_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.1;Ensure That Cloud Audit Logging Is Configured Properly;2 Logging and Monitoring;;Level 1;Automated;It is recommended that Cloud Audit Logging is configured to track all admin activities and read, write access to user data.;Cloud Audit Logging maintains two audit logs for each project, folder, and organization: Admin Activity and Data Access.1. Admin Activity logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity audit logs are enabled for all services and cannot be configured.2. Data Access audit logs record API calls that create, modify, or read user-provided data. These are disabled by default and should be enabled. There are three kinds of Data Access audit log information: - Admin read: Records operations that read metadata or configuration information. Admin Activity audit logs record writes of metadata and configuration information that cannot be disabled. - Data read: Records operations that read user-provided data. - Data write: Records operations that write user-provided data.It is recommended to have an effective default audit config configured in such a way that:1. logtype is set to DATA_READ (to log user activity tracking) and DATA_WRITES (to log changes/tampering to user data).2. audit config is enabled for all the services supported by the Data Access audit logs feature.3. Logs should be captured for all users, i.e., there are no exempted users in any of the audit config sections. This will ensure overriding the audit config will not contradict the requirement.;There is no charge for Admin Activity audit logs.Enabling the Data Access audit logs might result in your project being charged for the additional logs usage.;**From Google Cloud Console**1. Go to `Audit Logs` by visiting [https://console.cloud.google.com/iam-admin/audit](https://console.cloud.google.com/iam-admin/audit).2. Follow the steps at [https://cloud.google.com/logging/docs/audit/configure-data-access](https://cloud.google.com/logging/docs/audit/configure-data-access) to enable audit logs for all Google Cloud services. Ensure that no exemptions are allowed.**From Google Cloud CLI**1. To read the project's IAM policy and store it in a file run a command:```gcloud projects get-iam-policy PROJECT_ID > /tmp/project_policy.yaml```Alternatively, the policy can be set at the organization or folder level. If setting the policy at the organization level, it is not necessary to also set it for each folder or project.```gcloud organizations get-iam-policy ORGANIZATION_ID > /tmp/org_policy.yamlgcloud resource-manager folders get-iam-policy FOLDER_ID > /tmp/folder_policy.yaml```2. Edit policy in /tmp/policy.yaml, adding or changing only the audit logs configuration to:**Note: Admin Activity Logs are enabled by default, and cannot be disabled. So they are not listed in these configuration changes.**```auditConfigs:- auditLogConfigs: - logType: DATA_WRITE - logType: DATA_READ service: allServices```**Note:** `exemptedMembers:` is not set as audit logging should be enabled for all the users3. To write new IAM policy run command:```gcloud organizations set-iam-policy ORGANIZATION_ID /tmp/org_policy.yamlgcloud resource-manager folders set-iam-policy FOLDER_ID /tmp/folder_policy.yamlgcloud projects set-iam-policy PROJECT_ID /tmp/project_policy.yaml```If the preceding command reports a conflict with another change, then repeat these steps, starting with the first step.;"**From Google Cloud Console**1. Go to `Audit Logs` by visiting [https://console.cloud.google.com/iam-admin/audit](https://console.cloud.google.com/iam-admin/audit).2. Ensure that Admin Read, Data Write, and Data Read are enabled for all Google Cloud services and that no exemptions are allowed.**From Google Cloud CLI**1. List the Identity and Access Management (IAM) policies for the project, folder, or organization: ```gcloud organizations get-iam-policy ORGANIZATION_IDgcloud resource-manager folders get-iam-policy FOLDER_IDgcloud projects get-iam-policy PROJECT_ID```2. Policy should have a default auditConfigs section which has the logtype set to DATA_WRITES and DATA_READ for all services. Note that projects inherit settings from folders, which in turn inherit settings from the organization. When called, projects get-iam-policy, the result shows only the policies set in the project, not the policies inherited from the parent folder or organization. Nevertheless, if the parent folder has Cloud Audit Logging enabled, the project does as well. Sample output for default audit configs may look like this:``` auditConfigs: - auditLogConfigs: - logType: ADMIN_READ - logType: DATA_WRITE - logType: DATA_READ service: allServices```3. Any of the auditConfigs sections should not have parameter ""exemptedMembers:"" set, which will ensure that Logging is enabled for all users and no user is exempted.";'- Log type `DATA_READ` is equally important to that of `DATA_WRITE` to track detailed user activities.- BigQuery Data Access logs are handled differently from other data access logs. BigQuery logs are enabled by default and cannot be disabled. They do not count against logs allotment and cannot result in extra logs charges.;https://cloud.google.com/logging/docs/audit/:https://cloud.google.com/logging/docs/audit/configure-data-access;FAIL;Audit Logs are not enabled for project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;iam_audit_logs_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.13;Ensure Cloud Asset Inventory Is Enabled;2 Logging and Monitoring;;Level 1;Automated;GCP Cloud Asset Inventory is services that provides a historical view of GCP resources and IAM policies through a time-series database. The information recorded includes metadata on Google Cloud resources, metadata on policies set on Google Cloud projects or resources, and runtime information gathered within a Google Cloud resource.Cloud Asset Inventory Service (CAIS) API enablement is not required for operation of the service, but rather enables the mechanism for searching/exporting CAIS asset data directly.;The GCP resources and IAM policies captured by GCP Cloud Asset Inventory enables security analysis, resource change tracking, and compliance auditing.It is recommended GCP Cloud Asset Inventory be enabled for all GCP projects.;;**From Google Cloud Console**Enable the Cloud Asset API:1. Go to `API & Services/Library` by visiting [https://console.cloud.google.com/apis/library](https://console.cloud.google.com/apis/library)2. Search for `Cloud Asset API` and select the result for _Cloud Asset API_3. Click the `ENABLE` button.**From Google Cloud CLI**Enable the Cloud Asset API:1. Enable the Cloud Asset API through the services interface:```gcloud services enable cloudasset.googleapis.com```;**From Google Cloud Console**Ensure that the Cloud Asset API is enabled:1. Go to `API & Services/Library` by visiting [https://console.cloud.google.com/apis/library](https://console.cloud.google.com/apis/library)2. Search for `Cloud Asset API` and select the result for _Cloud Asset API_3. Ensure that `API Enabled` is displayed.**From Google Cloud CLI**Ensure that the Cloud Asset API is enabled:1. Query enabled services:```gcloud services list --enabled --filter=name:cloudasset.googleapis.com```If the API is listed, then it is enabled. If the response is `Listed 0 items` the API is not enabled.;Additional info- Cloud Asset Inventory only keeps a five-week history of Google Cloud asset metadata. If a longer history is desired, automation to export the history to Cloud Storage or BigQuery should be evaluated.Users need not enable CAI API if they don't have any plans to export.;https://cloud.google.com/asset-inventory/docs;PASS;Cloud Asset Inventory is enabled in project inbound-entity-461511-j4.;cloudasset.googleapis.com;Cloud Asset Inventory;iam_cloud_asset_inventory_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;1.6;Ensure That IAM Users Are Not Assigned the Service Account User or Service Account Token Creator Roles at Project Level;1 Identity and Access Management;;Level 1;Automated;It is recommended to assign the `Service Account User (iam.serviceAccountUser)` and `Service Account Token Creator (iam.serviceAccountTokenCreator)` roles to a user for a specific service account rather than assigning the role to a user at project level.;A service account is a special Google account that belongs to an application or a virtual machine (VM), instead of to an individual end-user. Application/VM-Instance uses the service account to call the service's Google API so that users aren't directly involved.In addition to being an identity, a service account is a resource that has IAM policies attached to it. These policies determine who can use the service account.Users with IAM roles to update the App Engine and Compute Engine instances (such as App Engine Deployer or Compute Instance Admin) can effectively run code as the service accounts used to run these instances, and indirectly gain access to all the resources for which the service accounts have access. Similarly, SSH access to a Compute Engine instance may also provide the ability to execute code as that instance/Service account.Based on business needs, there could be multiple user-managed service accounts configured for a project. Granting the `iam.serviceAccountUser` or `iam.serviceAccountTokenCreator` roles to a user for a project gives the user access to all service accounts in the project, including service accounts that may be created in the future. This can result in elevation of privileges by using service accounts and corresponding `Compute Engine instances`.In order to implement `least privileges` best practices, IAM users should not be assigned the `Service Account User` or `Service Account Token Creator` roles at the project level. Instead, these roles should be assigned to a user for a specific service account, giving that user access to the service account. The `Service Account User` allows a user to bind a service account to a long-running job service, whereas the `Service Account Token Creator` role allows a user to directly impersonate (or assert) the identity of a service account.;After revoking `Service Account User` or `Service Account Token Creator` roles at the project level from all impacted user account(s), these roles should be assigned to a user(s) for specific service account(s) according to business needs.;"**From Google Cloud Console**1. Go to the IAM page in the GCP Console by visiting: [https://console.cloud.google.com/iam-admin/iam](https://console.cloud.google.com/iam-admin/iam).2. Click on the filter table text bar. Type `Role: Service Account User`3. Click the `Delete Bin` icon in front of the role `Service Account User` for every user listed as a result of a filter.4. Click on the filter table text bar. Type `Role: Service Account Token Creator`5. Click the `Delete Bin` icon in front of the role `Service Account Token Creator` for every user listed as a result of a filter.**From Google Cloud CLI**1. Using a text editor, remove the bindings with the `roles/iam.serviceAccountUser` or `roles/iam.serviceAccountTokenCreator`. For example, you can use the iam.json file shown below as follows: { ""bindings"": [ { ""members"": [ ""serviceAccount:our-project-123@appspot.gserviceaccount.com"", ], ""role"": ""roles/appengine.appViewer"" }, { ""members"": [ ""user:email1@gmail.com"" ], ""role"": ""roles/owner"" }, { ""members"": [ ""serviceAccount:our-project-123@appspot.gserviceaccount.com"", ""serviceAccount:123456789012-compute@developer.gserviceaccount.com"" ], ""role"": ""roles/editor"" } ], ""etag"": ""BwUjMhCsNvY="" }2. Update the project's IAM policy:```gcloud projects set-iam-policy PROJECT_ID iam.json```";"**From Google Cloud Console**1. Go to the IAM page in the GCP Console by visiting [https://console.cloud.google.com/iam-admin/iam](https://console.cloud.google.com/iam-admin/iam)2. Click on the filter table text bar, Type `Role: Service Account User`.3. Ensure no user is listed as a result of the filter.4. Click on the filter table text bar, Type `Role: Service Account Token Creator`.3. Ensure no user is listed as a result of the filter.**From Google Cloud CLI**To ensure IAM users are not assigned Service Account User role at the project level:```gcloud projects get-iam-policy PROJECT_ID --format json | jq '.bindings[].role' | grep ""roles/iam.serviceAccountUser""gcloud projects get-iam-policy PROJECT_ID --format json | jq '.bindings[].role' | grep ""roles/iam.serviceAccountTokenCreator""```These commands should not return any output.";To assign the role `roles/iam.serviceAccountUser` or `roles/iam.serviceAccountTokenCreator` to a user role on a service account instead of a project:1. Go to [https://console.cloud.google.com/projectselector/iam-admin/serviceaccounts](https://console.cloud.google.com/projectselector/iam-admin/serviceaccounts)2. Select ` Target Project`3. Select `target service account`. Click `Permissions` on the top bar. It will open permission pane on right side of the page4. Add desired members with `Service Account User` or `Service Account Token Creator` role.;https://cloud.google.com/iam/docs/service-accounts:https://cloud.google.com/iam/docs/granting-roles-to-service-accounts:https://cloud.google.com/iam/docs/understanding-roles:https://cloud.google.com/iam/docs/granting-changing-revoking-access:https://console.cloud.google.com/iam-admin/iam;PASS;No IAM Users assigned to service roles at project level inbound-entity-461511-j4.;inbound-entity-461511-j4;inbound-entity-461511-j4;iam_no_service_roles_at_project_level;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;1.8;Ensure That Separation of Duties Is Enforced While Assigning Service Account Related Roles to Users;1 Identity and Access Management;;Level 2;Automated;It is recommended that the principle of 'Separation of Duties' is enforced while assigning service-account related roles to users.;The built-in/predefined IAM role `Service Account admin` allows the user/identity to create, delete, and manage service account(s).The built-in/predefined IAM role `Service Account User` allows the user/identity (with adequate privileges on Compute and App Engine) to assign service account(s) to Apps/Compute Instances.Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be able to complete a malicious action. In Cloud IAM - service accounts, this could be an action such as using a service account to access resources that user should not normally have access to.Separation of duties is a business control typically used in larger organizations, meant to help avoid security or privacy incidents and errors. It is considered best practice.No user should have `Service Account Admin` and `Service Account User` roles assigned at the same time.;The removed role should be assigned to a different user based on business needs.;**From Google Cloud Console**1. Go to `IAM & Admin/IAM` using `https://console.cloud.google.com/iam-admin/iam`.2. For any member having both `Service Account Admin` and `Service account User` roles granted/assigned, click the `Delete Bin` icon to remove either role from the member.Removal of a role should be done based on the business requirements.;"**From Google Cloud Console**1. Go to `IAM & Admin/IAM` using `https://console.cloud.google.com/iam-admin/iam`.2. Ensure no member has the roles `Service Account Admin` and `Service account User` assigned together.**From Google Cloud CLI**1. List all users and role assignments:```gcloud projects get-iam-policy [Project_ID] --format json | \ jq -r '[ ([""Service_Account_Admin_and_User""] | (., map(length*""-""))),  ( [ .bindings[] |  select(.role == ""roles/iam.serviceAccountAdmin"" or .role == ""roles/iam.serviceAccountUser"").members[] ] |  group_by(.) |  map({User: ., Count: length}) |  .[] |  select(.Count == 2).User |  unique ) ] |  .[] |  @tsv'```2. All common users listed under `Service_Account_Admin_and_User` are assigned both the `roles/iam.serviceAccountAdmin` and `roles/iam.serviceAccountUser` roles.";Users granted with Owner (roles/owner) and Editor (roles/editor) have privileges equivalent to `Service Account Admin` and `Service Account User`. To avoid the misuse, Owner and Editor roles should be granted to very limited users and Use of these primitive privileges should be minimal. These requirements are addressed in separate recommendations.;https://cloud.google.com/iam/docs/service-accounts:https://cloud.google.com/iam/docs/understanding-roles:https://cloud.google.com/iam/docs/granting-roles-to-service-accounts;PASS;Principle of separation of duties was enforced for KMS-Related Roles in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;iam_role_kms_enforce_separation_of_duties;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;1.11;Ensure That Separation of Duties Is Enforced While Assigning KMS Related Roles to Users;1 Identity and Access Management;;Level 2;Automated;It is recommended that the principle of 'Separation of Duties' is enforced while assigning KMS related roles to users.;The built-in/predefined IAM role `Cloud KMS Admin` allows the user/identity to create, delete, and manage service account(s).The built-in/predefined IAM role `Cloud KMS CryptoKey Encrypter/Decrypter` allows the user/identity (with adequate privileges on concerned resources) to encrypt and decrypt data at rest using an encryption key(s).The built-in/predefined IAM role `Cloud KMS CryptoKey Encrypter` allows the user/identity (with adequate privileges on concerned resources) to encrypt data at rest using an encryption key(s).The built-in/predefined IAM role `Cloud KMS CryptoKey Decrypter` allows the user/identity (with adequate privileges on concerned resources) to decrypt data at rest using an encryption key(s).Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be able to complete a malicious action. In Cloud KMS, this could be an action such as using a key to access and decrypt data a user should not normally have access to. Separation of duties is a business control typically used in larger organizations, meant to help avoid security or privacy incidents and errors. It is considered best practice.No user(s) should have `Cloud KMS Admin` and any of the `Cloud KMS CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, `Cloud KMS CryptoKey Decrypter` roles assigned at the same time.;Removed roles should be assigned to another user based on business needs.;**From Google Cloud Console**1. Go to `IAM & Admin/IAM` using `https://console.cloud.google.com/iam-admin/iam`2. For any member having `Cloud KMS Admin` and any of the `Cloud KMS CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, `Cloud KMS CryptoKey Decrypter` roles granted/assigned, click the `Delete Bin` icon to remove the role from the member.Note: Removing a role should be done based on the business requirement.;**From Google Cloud Console**1. Go to `IAM & Admin/IAM` by visiting: [https://console.cloud.google.com/iam-admin/iam](https://console.cloud.google.com/iam-admin/iam)2. Ensure no member has the roles `Cloud KMS Admin` and any of the `Cloud KMS CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, `Cloud KMS CryptoKey Decrypter` assigned.**From Google Cloud CLI**1. List all users and role assignments:```gcloud projects get-iam-policy PROJECT_ID```2. Ensure that there are no common users found in the member section for roles `cloudkms.admin` and any one of `Cloud KMS CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, `Cloud KMS CryptoKey Decrypter`;Users granted with Owner (roles/owner) and Editor (roles/editor) have privileges equivalent to `Cloud KMS Admin` and `Cloud KMS CryptoKey Encrypter/Decrypter`. To avoid misuse, Owner and Editor roles should be granted to a very limited group of users. Use of these primitive privileges should be minimal.;https://cloud.google.com/kms/docs/separation-of-duties;PASS;Principle of separation of duties was enforced for KMS-Related Roles in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;iam_role_kms_enforce_separation_of_duties;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.5;Ensure That the Log Metric Filter and Alerts Exist for Audit Configuration Changes;2 Logging and Monitoring;;Level 1;Automated;"Google Cloud Platform (GCP) services write audit log entries to the Admin Activity and Data Access logs to help answer the questions of, ""who did what, where, and when?"" within GCP projects.Cloud audit logging records information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by GCP services. Cloud audit logging provides a history of GCP API calls for an account, including API calls made via the console, SDKs, command-line tools, and other GCP services.";Admin activity and data access logs produced by cloud audit logging enable security analysis, resource change tracking, and compliance auditing.Configuring the metric filter and alerts for audit configuration changes ensures the recommended state of audit configuration is maintained so that all activities in the project are audit-able at any point in time.;Enabling of logging may result in your project being charged for the additional logs usage.;"**From Google Cloud Console****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```protoPayload.methodName=""SetIamPolicy"" ANDprotoPayload.serviceData.policyDelta.auditConfigDeltas:*```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This will ensure that the log metric counts the number of log entries matching the user's advanced logs query.6. Click `Create Metric`. **Create a prescribed Alert Policy:** 1. Identify the new metric the user just created, under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page opens.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notifications channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create a prescribed Log Metric:- Use the command: gcloud beta logging metrics create - Reference for command usage: [https://cloud.google.com/sdk/gcloud/reference/beta/logging/metrics/create](https://cloud.google.com/sdk/gcloud/reference/beta/logging/metrics/create)Create prescribed Alert Policy - Use the command: gcloud alpha monitoring policies create- Reference for command usage: [https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create](https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create)";"**From Google Cloud Console****Ensure the prescribed log metric is present:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure that at least one metric `<Log_Metric_Name>` is present with the filter text:```protoPayload.methodName=""SetIamPolicy"" ANDprotoPayload.serviceData.policyDelta.auditConfigDeltas:*```**Ensure that the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of 0 for greater than zero(0) seconds`, means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that appropriate notifications channels have been set up.**From Google Cloud CLI****Ensure that the prescribed log metric is present:**1. List the log metrics:```gcloud beta logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to: ```protoPayload.methodName=""SetIamPolicy"" ANDprotoPayload.serviceData.policyDelta.auditConfigDeltas:*```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains at least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/logging/docs/audit/configure-data-access#getiampolicy-setiampolicy;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_audit_configuration_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.10;Ensure That the Log Metric Filter and Alerts Exist for Cloud Storage IAM Permission Changes;2 Logging and Monitoring;;Level 2;Automated;It is recommended that a metric filter and alarm be established for Cloud Storage Bucket IAM changes.;Monitoring changes to cloud storage bucket permissions may reduce the time needed to detect and correct permissions on sensitive cloud storage buckets and objects inside the bucket.;Enabling of logging may result in your project being charged for the additional logs usage. These charges could be significant depending on the size of the organization.;"**From Google Cloud Console****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```resource.type=""gcs_bucket"" AND protoPayload.methodName=""storage.setIamPermissions""```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.6. Click `Create Metric`. **Create the prescribed Alert Policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page appears.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notifications channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed Log Metric:- Use the command: gcloud beta logging metrics create Create the prescribed alert policy: - Use the command: gcloud alpha monitoring policies create";"**From Google Cloud Console****Ensure the prescribed log metric is present:**1. For each project that contains cloud storage buckets, go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure at least one metric `<Log_Metric_Name>` is present with the filter text:```resource.type=""gcs_bucket""AND protoPayload.methodName=""storage.setIamPermissions""```**Ensure that the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of 0 for greater than 0 seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that the appropriate notifications channels have been set up.**From Google Cloud CLI****Ensure that the prescribed log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to: ```resource.type=gcs_bucket AND protoPayload.methodName=""storage.setIamPermissions""```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains an least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/storage/docs/overview:https://cloud.google.com/storage/docs/access-control/iam-roles;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_bucket_permission_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.6;Ensure That the Log Metric Filter and Alerts Exist for Custom Role Changes;2 Logging and Monitoring;;Level 1;Automated;It is recommended that a metric filter and alarm be established for changes to Identity and Access Management (IAM) role creation, deletion and updating activities.;Google Cloud IAM provides predefined roles that give granular access to specific Google Cloud Platform resources and prevent unwanted access to other resources. However, to cater to organization-specific needs, Cloud IAM also provides the ability to create custom roles. Project owners and administrators with the Organization Role Administrator role or the IAM Role Administrator role can create custom roles. Monitoring role creation, deletion and updating activities will help in identifying any over-privileged role at early stages.;Enabling of logging may result in your project being charged for the additional logs usage.;"**From Console:****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".1. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.1. Clear any text and add: ```resource.type=""iam_role"" AND (protoPayload.methodName = ""google.iam.admin.v1.CreateRole"" OR protoPayload.methodName=""google.iam.admin.v1.DeleteRole"" OR protoPayload.methodName=""google.iam.admin.v1.UpdateRole"")```1. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.1. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the advanced logs query.1. Click `Create Metric`. **Create a prescribed Alert Policy:** 1. Identify the new metric that was just created under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the metric and select `Create alert from Metric`. A new page displays.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```1. Configure the desired notification channels in the section `Notifications`.1. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed Log Metric:- Use the command: gcloud logging metrics create Create the prescribed Alert Policy: - Use the command: gcloud alpha monitoring policies create <policy name>";"**From Console:****Ensure that the prescribed log metric is present:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure that at least one metric `<Log_Metric_Name>` is present with filter text:```resource.type=""iam_role"" AND (protoPayload.methodName=""google.iam.admin.v1.CreateRole"" OR protoPayload.methodName=""google.iam.admin.v1.DeleteRole"" OR protoPayload.methodName=""google.iam.admin.v1.UpdateRole"")```**Ensure that the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of zero(0) for greater than zero(0) seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that the appropriate notifications channels have been set up.**From Google Cloud CLI**Ensure that the prescribed log metric is present:1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to:```resource.type=""iam_role""AND (protoPayload.methodName = ""google.iam.admin.v1.CreateRole"" ORprotoPayload.methodName=""google.iam.admin.v1.DeleteRole"" ORprotoPayload.methodName=""google.iam.admin.v1.UpdateRole"")```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains an least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`.";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/iam/docs/understanding-custom-roles;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_custom_role_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.4;Ensure Log Metric Filter and Alerts Exist for Project Ownership Assignments/Changes;2 Logging and Monitoring;;Level 1;Automated;In order to prevent unnecessary project ownership assignments to users/service-accounts and further misuses of projects and resources, all `roles/Owner` assignments should be monitored.Members (users/Service-Accounts) with a role assignment to primitive role `roles/Owner` are project owners.The project owner has all the privileges on the project the role belongs to. These are summarized below:- All viewer permissions on all GCP Services within the project- Permissions for actions that modify the state of all GCP services within the project- Manage roles and permissions for a project and all resources within the project- Set up billing for a projectGranting the owner role to a member (user/Service-Account) will allow that member to modify the Identity and Access Management (IAM) policy. Therefore, grant the owner role only if the member has a legitimate purpose to manage the IAM policy. This is because the project IAM policy contains sensitive access control data. Having a minimal set of users allowed to manage IAM policy will simplify any auditing that may be necessary.;Project ownership has the highest level of privileges on a project. To avoid misuse of project resources, the project ownership assignment/change actions mentioned above should be monitored and alerted to concerned recipients.- Sending project ownership invites- Acceptance/Rejection of project ownership invite by user- Adding `role\Owner` to a user/service-account- Removing a user/Service account from `role\Owner`;Enabling of logging may result in your project being charged for the additional logs usage.;"**From Google Cloud Console****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```(protoPayload.serviceName=""cloudresourcemanager.googleapis.com"") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""REMOVE"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""ADD"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"")```4. Click `Submit Filter`. The logs display based on the filter text entered by the user.5. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and the `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the advanced logs query.6. Click `Create Metric`. **Create the display prescribed Alert Policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the desired metric and select `Create alert from Metric`. A new page opens.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notifications channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create a prescribed Log Metric:- Use the command: gcloud beta logging metrics create - Reference for Command Usage: https://cloud.google.com/sdk/gcloud/reference/beta/logging/metrics/createCreate prescribed Alert Policy - Use the command: gcloud alpha monitoring policies create- Reference for Command Usage: https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create";"**From Google Cloud Console****Ensure that the prescribed log metric is present:**1. Go to `Logging/Log-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure that at least one metric `<Log_Metric_Name>` is present with filter text:```(protoPayload.serviceName=""cloudresourcemanager.googleapis.com"") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""REMOVE"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""ADD"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"")```**Ensure that the prescribed Alerting Policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of zero(0) for greater than zero(0) seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for your organization.5. Ensure that the appropriate notifications channels have been set up.**From Google Cloud CLI****Ensure that the prescribed log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with filter set to: ```(protoPayload.serviceName=""cloudresourcemanager.googleapis.com"") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""REMOVE"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=""ADD"" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=""roles/owner"")```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains an least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";1. Project ownership assignments for a user cannot be done using the gcloud utility as assigning project ownership to a user requires sending, and the user accepting, an invitation. 2. Project Ownership assignment to a service account does not send any invites. SetIAMPolicy to `role/owner`is directly performed on service accounts.;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_project_ownership_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.11;Ensure That the Log Metric Filter and Alerts Exist for SQL Instance Configuration Changes;2 Logging and Monitoring;;Level 2;Automated;It is recommended that a metric filter and alarm be established for SQL instance configuration changes.;Monitoring changes to SQL instance configuration changes may reduce the time needed to detect and correct misconfigurations done on the SQL server. Below are a few of the configurable options which may the impact security posture of an SQL instance:- Enable auto backups and high availability: Misconfiguration may adversely impact business continuity, disaster recovery, and high availability - Authorize networks: Misconfiguration may increase exposure to untrusted networks;Enabling of logging may result in your project being charged for the additional logs usage. These charges could be significant depending on the size of the organization.;"**From Google Cloud Console****Create the prescribed Log Metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```protoPayload.methodName=""cloudsql.instances.update""```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.6. Click `Create Metric`. **Create the prescribed alert policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page appears.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the user's project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notification channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed log metric:- Use the command: gcloud logging metrics create Create the prescribed alert policy: - Use the command: gcloud alpha monitoring policies create- Reference for command usage: [https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create](https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create)";"**From Google Cloud Console****Ensure the prescribed log metric is present:**1. For each project that contains Cloud SQL instances, go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure that at least one metric `<Log_Metric_Name>` is present with the filter text:```protoPayload.methodName=""cloudsql.instances.update""```**Ensure that the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of zero(0) for greater than zero(0) seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that the appropriate notifications channels have been set up.**From Google Cloud CLI****Ensure that the prescribed log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to ```protoPayload.methodName=""cloudsql.instances.update""```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains at least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/storage/docs/overview:https://cloud.google.com/sql/docs/:https://cloud.google.com/sql/docs/mysql/:https://cloud.google.com/sql/docs/postgres/;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_sql_instance_configuration_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.7;Ensure That the Log Metric Filter and Alerts Exist for VPC Network Firewall Rule Changes;2 Logging and Monitoring;;Level 2;Automated;It is recommended that a metric filter and alarm be established for Virtual Private Cloud (VPC) Network Firewall rule changes.;Monitoring for Create or Update Firewall rule events gives insight to network access changes and may reduce the time it takes to detect suspicious activity.;Enabling of logging may result in your project being charged for the additional logs usage. These charges could be significant depending on the size of the organization.;"**From Google Cloud Console****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```resource.type=""gce_firewall_rule"" AND (protoPayload.methodName:""compute.firewalls.patch"" OR protoPayload.methodName:""compute.firewalls.insert""OR protoPayload.methodName:""compute.firewalls.delete"")```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the advanced logs query.6. Click `Create Metric`. **Create the prescribed Alert Policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page displays.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notifications channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed Log Metric- Use the command: gcloud logging metrics create Create the prescribed alert policy: - Use the command: gcloud alpha monitoring policies create";"**From Google Cloud Console****Ensure that the prescribed log metric is present:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure at least one metric `<Log_Metric_Name>` is present with this filter text:```resource.type=""gce_firewall_rule"" AND (protoPayload.methodName:""compute.firewalls.patch"" OR protoPayload.methodName:""compute.firewalls.insert""OR protoPayload.methodName:""compute.firewalls.delete"")```**Ensure that the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of zero(0) for greater than zero(0) seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that appropriate notification channels have been set up.**From Google Cloud CLI****Ensure that the prescribed log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to: ```resource.type=""gce_firewall_rule"" AND (protoPayload.methodName:""compute.firewalls.patch"" OR protoPayload.methodName:""compute.firewalls.insert""OR protoPayload.methodName:""compute.firewalls.delete"")```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains an least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/vpc/docs/firewalls;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_vpc_firewall_rule_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.9;Ensure That the Log Metric Filter and Alerts Exist for VPC Network Changes;2 Logging and Monitoring;;Level 2;Automated;It is recommended that a metric filter and alarm be established for Virtual Private Cloud (VPC) network changes.;It is possible to have more than one VPC within a project. In addition, it is also possible to create a peer connection between two VPCs enabling network traffic to route between VPCs. Monitoring changes to a VPC will help ensure VPC traffic flow is not getting impacted.;Enabling of logging may result in your project being charged for the additional logs usage. These charges could be significant depending on the size of the organization.;"**From Google Cloud Console****Create the prescribed log metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`.3. Clear any text and add: ```resource.type=""gce_network"" AND (protoPayload.methodName:""compute.networks.insert"" OR protoPayload.methodName:""compute.networks.patch"" OR protoPayload.methodName:""compute.networks.delete"" OR protoPayload.methodName:""compute.networks.removePeering"" OR protoPayload.methodName:""compute.networks.addPeering"")```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.6. Click `Create Metric`. **Create the prescribed alert policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page appears.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of 0 for the most recent value will ensure that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notification channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed Log Metric:- Use the command: gcloud logging metrics create Create the prescribed alert policy: - Use the command: gcloud alpha monitoring policies create";"**From Google Cloud Console****Ensure the prescribed log metric is present:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure at least one metric `<Log_Metric_Name>` is present with filter text:```resource.type=""gce_network"" AND (protoPayload.methodName:""compute.networks.insert"" OR protoPayload.methodName:""compute.networks.patch"" OR protoPayload.methodName:""compute.networks.delete"" OR protoPayload.methodName:""compute.networks.removePeering"" OR protoPayload.methodName:""compute.networks.addPeering"")```**Ensure the prescribed alerting policy is present:**3. Go to `Alerting` by visiting [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of 0 for greater than 0 seconds` means that the alert will trigger for any new owner change. Verify that the chosen alerting thresholds make sense for the user's organization.5. Ensure that appropriate notification channels have been set up.**From Google Cloud CLI****Ensure the log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with filter set to: ```resource.type=""gce_network"" AND protoPayload.methodName=""beta.compute.networks.insert"" OR protoPayload.methodName=""beta.compute.networks.patch"" OR protoPayload.methodName=""v1.compute.networks.delete"" OR protoPayload.methodName=""v1.compute.networks.removePeering"" OR protoPayload.methodName=""v1.compute.networks.addPeering""```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains at least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/vpc/docs/overview;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_vpc_network_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;inbound-entity-461511-j4;global;2025-07-01 12:41:26.937059;2.8;Ensure That the Log Metric Filter and Alerts Exist for VPC Network Route Changes;2 Logging and Monitoring;;Level 2;Automated;It is recommended that a metric filter and alarm be established for Virtual Private Cloud (VPC) network route changes.;Google Cloud Platform (GCP) routes define the paths network traffic takes from a VM instance to another destination. The other destination can be inside the organization VPC network (such as another VM) or outside of it. Every route consists of a destination and a next hop. Traffic whose destination IP is within the destination range is sent to the next hop for delivery. Monitoring changes to route tables will help ensure that all VPC traffic flows through an expected path.;Enabling of logging may result in your project being charged for the additional logs usage. These charges could be significant depending on the size of the organization.;"**From Google Cloud Console****Create the prescribed Log Metric:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics) and click ""CREATE METRIC"".2. Click the down arrow symbol on the `Filter Bar` at the rightmost corner and select `Convert to Advanced Filter`3. Clear any text and add: ```resource.type=""gce_route"" AND (protoPayload.methodName:""compute.routes.delete"" OR protoPayload.methodName:""compute.routes.insert"")```4. Click `Submit Filter`. Display logs appear based on the filter text entered by the user.5. In the `Metric Editor` menu on the right, fill out the name field. Set `Units` to `1` (default) and `Type` to `Counter`. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.6. Click `Create Metric`. **Create the prescribed alert policy:** 1. Identify the newly created metric under the section `User-defined Metrics` at [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. Click the 3-dot icon in the rightmost column for the new metric and select `Create alert from Metric`. A new page displays.3. Fill out the alert policy configuration and click `Save`. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:```Set `Aggregator` to `Count`Set `Configuration`:- Condition: above- Threshold: 0- For: most recent value```4. Configure the desired notification channels in the section `Notifications`.5. Name the policy and click `Save`.**From Google Cloud CLI**Create the prescribed Log Metric:- Use the command: gcloud logging metrics create Create the prescribed the alert policy: - Use the command: gcloud alpha monitoring policies create";"**From Google Cloud Console****Ensure that the prescribed Log metric is present:**1. Go to `Logging/Logs-based Metrics` by visiting [https://console.cloud.google.com/logs/metrics](https://console.cloud.google.com/logs/metrics).2. In the `User-defined Metrics` section, ensure that at least one metric `<Log_Metric_Name>` is present with the filter text:```resource.type=""gce_route"" AND (protoPayload.methodName:""compute.routes.delete"" OR protoPayload.methodName:""compute.routes.insert"")```**Ensure the prescribed alerting policy is present:**3. Go to `Alerting` by visiting: [https://console.cloud.google.com/monitoring/alerting](https://console.cloud.google.com/monitoring/alerting).4. Under the `Policies` section, ensure that at least one alert policy exists for the log metric above. Clicking on the policy should show that it is configured with a condition. For example, `Violates when: Any logging.googleapis.com/user/<Log Metric Name> stream` `is above a threshold of 0 for greater than zero(0) seconds` means that the alert will trigger for any new owner change. Verify that the chosen alert thresholds make sense for the user's organization.5. Ensure that the appropriate notification channels have been set up.**From Google Cloud CLI****Ensure the prescribed log metric is present:**1. List the log metrics:```gcloud logging metrics list --format json```2. Ensure that the output contains at least one metric with the filter set to: ```resource.type=""gce_route"" AND (protoPayload.methodName:""compute.routes.delete"" OR protoPayload.methodName:""compute.routes.insert"")```3. Note the value of the property `metricDescriptor.type` for the identified metric, in the format `logging.googleapis.com/user/<Log Metric Name>`.**Ensure that the prescribed alerting policy is present:**4. List the alerting policies:```gcloud alpha monitoring policies list --format json```5. Ensure that the output contains an least one alert policy where:- `conditions.conditionThreshold.filter` is set to `metric.type=\""logging.googleapis.com/user/<Log Metric Name>\""`- AND `enabled` is set to `true`";;https://cloud.google.com/logging/docs/logs-based-metrics/:https://cloud.google.com/monitoring/custom-metrics/:https://cloud.google.com/monitoring/alerts/:https://cloud.google.com/logging/docs/reference/tools/gcloud-logging:https://cloud.google.com/storage/docs/access-control/iam:https://cloud.google.com/sdk/gcloud/reference/beta/logging/metrics/create:https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create;FAIL;There are no log metric filters or alerts associated in project inbound-entity-461511-j4.;inbound-entity-461511-j4;My First Project;logging_log_metric_filter_and_alert_for_vpc_network_route_changes_enabled;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;1.1;Ensure that Corporate Login Credentials are Used;1 Identity and Access Management;;Level 1;Manual;Use corporate login credentials instead of consumer accounts, such as Gmail accounts.;It is recommended fully-managed corporate Google accounts be used for increased visibility, auditing, and controlling access to Cloud Platform resources. Email accounts based outside of the user's organization, such as consumer accounts, should not be used for business purposes.;There will be increased overhead as maintaining accounts will now be required. For smaller organizations, this will not be an issue, but will balloon with size.;Remove all consumer Google accounts from IAM policies. Follow the documentation and setup corporate login accounts.**Prevention:**To ensure that no email addresses outside the organization can be granted IAM permissions to its Google Cloud projects, folders or organization, turn on the Organization Policy for `Domain Restricted Sharing`. Learn more at: [https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains](https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains);For each Google Cloud Platform project, list the accounts that have been granted access to that project:**From Google Cloud CLI**```gcloud projects get-iam-policy PROJECT_ID```Also list the accounts added on each folder: ```gcloud resource-manager folders get-iam-policy FOLDER_ID ```And list your organization's IAM policy: ```gcloud organizations get-iam-policy ORGANIZATION_ID```No email accounts outside the organization domain should be granted permissions in the IAM policies. This excludes Google-owned service accounts.;;https://support.google.com/work/android/answer/6371476:https://cloud.google.com/sdk/gcloud/reference/projects/get-iam-policy:https://cloud.google.com/sdk/gcloud/reference/resource-manager/folders/get-iam-policy:https://cloud.google.com/sdk/gcloud/reference/organizations/get-iam-policy:https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains:https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;1.2;Ensure that Multi-Factor Authentication is 'Enabled' for All Non-Service Accounts;1 Identity and Access Management;;Level 1;Manual;Setup multi-factor authentication for Google Cloud Platform accounts.;Multi-factor authentication requires more than one mechanism to authenticate a user. This secures user logins from attackers exploiting stolen or weak credentials.;;**From Google Cloud Console**For each Google Cloud Platform project:1. Identify non-service accounts.1. Setup multi-factor authentication for each account.;**From Google Cloud Console**For each Google Cloud Platform project, folder, or organization:1. Identify non-service accounts.1. Manually verify that multi-factor authentication for each account is set.;;https://cloud.google.com/solutions/securing-gcp-account-u2f:https://support.google.com/accounts/answer/185839;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;1.3;Ensure that Security Key Enforcement is Enabled for All Admin Accounts;1 Identity and Access Management;;Level 2;Manual;Setup Security Key Enforcement for Google Cloud Platform admin accounts.;Google Cloud Platform users with Organization Administrator roles have the highest level of privilege in the organization. These accounts should be protected with the strongest form of two-factor authentication: Security Key Enforcement. Ensure that admins use Security Keys to log in instead of weaker second factors like SMS or one-time passwords (OTP). Security Keys are actual physical keys used to access Google Organization Administrator Accounts. They send an encrypted signature rather than a code, ensuring that logins cannot be phished.;If an organization administrator loses access to their security key, the user could lose access to their account. For this reason, it is important to set up backup security keys.;1. Identify users with the Organization Administrator role.2. Setup Security Key Enforcement for each account. Learn more at: [https://cloud.google.com/security-key/](https://cloud.google.com/security-key/);"1. Identify users with Organization Administrator privileges:```gcloud organizations get-iam-policy ORGANIZATION_ID```Look for members granted the role ""roles/resourcemanager.organizationAdmin"".2. Manually verify that Security Key Enforcement has been enabled for each account.";;https://cloud.google.com/security-key/:https://gsuite.google.com/learn-more/key_for_working_smarter_faster_and_more_securely.html;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;1.13;Ensure API Keys Are Restricted To Use by Only Specified Hosts and Apps;1 Identity and Access Management;;Level 2;Manual;API Keys should only be used for services in cases where other authentication methods are unavailable. In this case, unrestricted keys are insecure because they can be viewed publicly, such as from within a browser, or they can be accessed on a device where the key resides. It is recommended to restrict API key usage to trusted hosts, HTTP referrers and apps. It is recommended to use the more secure standard authentication flow instead.;Security risks involved in using API-Keys appear below:- API keys are simple encrypted strings- API keys do not identify the user or the application making the API request- API keys are typically accessible to clients, making it easy to discover and steal an API keyIn light of these potential risks, Google recommends using the standard authentication flow instead of API keys. However, there are limited cases where API keys are more appropriate. For example, if there is a mobile application that needs to use the Google Cloud Translation API, but doesn't otherwise need a backend server, API keys are the simplest way to authenticate to that API.In order to reduce attack vectors, API-Keys can be restricted only to trusted hosts, HTTP referrers and applications.;Setting `Application Restrictions` may break existing application functioning, if not done carefully.;**From Google Cloud Console*****Leaving Keys in Place***1. Go to `APIs & Services\Credentials` using `https://console.cloud.google.com/apis/credentials`2. In the section `API Keys`, Click the `API Key Name`. The API Key properties display on a new page.3. In the `Key restrictions` section, set the application restrictions to any of `HTTP referrers, IP addresses, Android apps, iOS apps`.4. Click `Save`.1. Repeat steps 2,3,4 for every unrestricted API key.**Note:** Do not set `HTTP referrers` to wild-cards (* or *.[TLD] or *.[TLD]/*) allowing access to any/wide HTTP referrer(s)Do not set `IP addresses` and referrer to `any host (0.0.0.0 or 0.0.0.0/0 or ::0)`***Removing Keys***Another option is to remove the keys entirely.1. Go to `APIs & Services\Credentials` using `https://console.cloud.google.com/apis/credentials`2. In the section `API Keys`, select the checkbox next to each key you wish to remove3. Select `Delete` and confirm.;"**From Google Cloud Console**1. Go to `APIs & Services\Credentials` using `https://console.cloud.google.com/apis/credentials`1. In the section `API Keys`, Click the `API Key Name`. The API Key properties display on a new page.1. For every API Key, ensure the section `Key restrictions` parameter `Application restrictions` is not set to `None`.Or,1. Ensure `Application restrictions` is set to `HTTP referrers` and the referrer is not set to wild-cards `(* or *.[TLD] or *.[TLD]/*) allowing access to any/wide HTTP referrer(s)`Or,1. Ensure `Application restrictions` is set to `IP addresses` and referrer is not set to `any host (0.0.0.0 or 0.0.0.0/0 or ::0)`**From Google Cloud Command Line**1. Run the following from within the project you wish to audit ```gcloud services api-keys list --filter=""-restrictions:*"" --format=""table[box](displayName:label='Key With No Restrictions')```";;https://cloud.google.com/docs/authentication/api-keys:https://cloud.google.com/sdk/gcloud/reference/services/api-keys/list:https://cloud.google.com/sdk/gcloud/reference/alpha/services/api-keys/update;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;1.17;Ensure Secrets are Not Stored in Cloud Functions Environment Variables by Using Secret Manager;1 Identity and Access Management;;Level 1;Manual;Google Cloud Functions allow you to host serverless code that is executed when an event is triggered, without the requiring the management a host operating system. These functions can also store environment variables to be used by the code that may contain authentication or other information that needs to remain confidential.;It is recommended to use the Secret Manager, because environment variables are stored unencrypted, and accessible for all users who have access to the code.;There should be no impact on the Cloud Function. There are minor costs after 10,000 requests a month to the Secret Manager API as well for a high use of other functions. Modifying the Cloud Function to use the Secret Manager may prevent it running to completion.;"Enable Secret Manager API for your Project**From Google Cloud Console**1. Within the project you wish to enable, select the Navigation hamburger menu in the top left. Hover over 'APIs & Services' to under the heading 'Serverless', then select 'Enabled APIs & Services' in the menu that opens up.2. Click the button '+ Enable APIS and Services'3. In the Search bar, search for 'Secret Manager API' and select it.4. Click the blue box that says 'Enable'.**From Google Cloud CLI**1. Within the project you wish to enable the API in, run the following command.```gcloud services enable Secret Manager API ```Reviewing Environment Variables That Should Be Migrated to Secret Manager**From Google Cloud Console**1. Log in to the Google Cloud Web Portal (https://console.cloud.google.com/)1. Go to Cloud Functions1. Click on a function name from the list1. Click on Edit and review the Runtime environment for variables that should be secrets. Leave this list open for the next step.**From Google Cloud CLI**1. To view a list of your cloud functions run```gcloud functions list```2. For each cloud function run the following command.```gcloud functions describe <function_name>```3. Review the settings of the buildEnvironmentVariables and environmentVariables. Keep this information for the next step.Migrating Environment Variables to Secrets within the Secret Manager**From Google Cloud Console**1. Go to the Secret Manager page in the Cloud Console.1. On the Secret Manager page, click Create Secret.1. On the Create secret page, under Name, enter the name of the Environment Variable you are replacing. This will then be the Secret Variable you will reference in your code.1. You will also need to add a version. This is the actual value of the variable that will be referenced from the code. To add a secret version when creating the initial secret, in the Secret value field, enter the value from the Environment Variable you are replacing.1. Leave the Regions section unchanged.1. Click the Create secret button.1. Repeat for all Environment Variables**From Google Cloud CLI**1. Run the following command with the Environment Variable name you are replacing in the `<secret-id>`. It is most secure to point this command to a file with the Environment Variable value located in it, as if you entered it via command line it would show up in your shell’s command history.```gcloud secrets create <secret-id> --data-file=""/path/to/file.txt""```Granting your Runtime's Service Account Access to Secrets**From Google Cloud Console**1. Within the project containing your runtime login with account that has the 'roles/secretmanager.secretAccessor' permission. 2. Select the Navigation hamburger menu in the top left. Hover over 'Security' to under the then select 'Secret Manager' in the menu that opens up.3. Click the name of a secret listed in this screen.4. If it is not already open, click Show Info Panel in this screen to open the panel.5.In the info panel, click Add principal.6.In the New principals field, enter the service account your function uses for its identity. (If you need help locating or updating your runtime's service account, please see the 'docs/securing/function-identity#runtime_service_account' reference.)7. In the Select a role dropdown, choose Secret Manager and then Secret Manager Secret Accessor.**From Google Cloud CLI**As of the time of writing, using Google CLI to list Runtime variables is only in beta. Because this is likely to change we are not including it here.Modifying the Code to use the Secrets in Secret Manager**From Google Cloud Console**This depends heavily on which language your runtime is in. For the sake of the brevity of this recommendation, please see the '/docs/creating-and-accessing-secrets#access' reference for language specific instructions.**From Google Cloud CLI**This depends heavily on which language your runtime is in. For the sake of the brevity of this recommendation, please see the' /docs/creating-and-accessing-secrets#access' reference for language specific instructions.Deleting the Insecure Environment Variables**Be certain to do this step last.** Removing variables from code actively referencing them will prevent it from completing successfully.**From Google Cloud Console**1. Select the Navigation hamburger menu in the top left. Hover over 'Security' then select 'Secret Manager' in the menu that opens up.1. Click the name of a function. Click Edit.1. Click Runtime, build and connections settings to expand the advanced configuration options.1. Click 'Security’. Hover over the secret you want to remove, then click 'Delete'.1. Click Next. Click Deploy. The latest version of the runtime will now reference the secrets in Secret Manager.**From Google Cloud CLI**```gcloud functions deploy <Function name>--remove-env-vars <env vars>```If you need to find the env vars to remove, they are from the step where ‘gcloud functions describe `<function_name>`’ was run.";Determine if Confidential Information is Stored in your Functions in Cleartext**From Google Cloud Console**1. Within the project you wish to audit, select the Navigation hamburger menu in the top left. Scroll down to under the heading 'Serverless', then select 'Cloud Functions'1. Click on a function name from the list1. Open the Variables tab and you will see both buildEnvironmentVariables and environmentVariables1. Review the variables whether they are secrets1. Repeat step 3-5 until all functions are reviewed**From Google Cloud CLI**1. To view a list of your cloud functions run```gcloud functions list```2. For each cloud function in the list run the following command.```gcloud functions describe <function_name>```3. Review the settings of the buildEnvironmentVariables and environmentVariables. Determine if this is data that should not be publicly accessible.Determine if Secret Manager API is 'Enabled' for your Project**From Google Cloud Console**1. Within the project you wish to audit, select the Navigation hamburger menu in the top left. Hover over 'APIs & Services' to under the heading 'Serverless', then select 'Enabled APIs & Services' in the menu that opens up.1. Click the button '+ Enable APIS and Services'1. In the Search bar, search for 'Secret Manager API' and select it.1. If it is enabled, the blue box that normally says 'Enable' will instead say 'Manage'.**From Google Cloud CLI**1. Within the project you wish to audit, run the following command.```gcloud services list```2. If 'Secret Manager API' is in the list, it is enabled.;There are slight additional costs to using the Secret Manager API. Review the documentation to determine your organizations' needs.;https://cloud.google.com/functions/docs/configuring/env-var#managing_secrets:https://cloud.google.com/secret-manager/docs/overview;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;2.12;Ensure That Cloud DNS Logging Is Enabled for All VPC Networks;2 Logging and Monitoring;;Level 1;Automated;Cloud DNS logging records the queries from the name servers within your VPC to Stackdriver. Logged queries can come from Compute Engine VMs, GKE containers, or other GCP resources provisioned within the VPC.;Security monitoring and forensics cannot depend solely on IP addresses from VPC flow logs, especially when considering the dynamic IP usage of cloud resources, HTTP virtual host routing, and other technology that can obscure the DNS name used by a client from the IP address. Monitoring of Cloud DNS logs provides visibility to DNS names requested by the clients within the VPC. These logs can be monitored for anomalous domain names, evaluated against threat intelligence, and Note: For full capture of DNS, firewall must block egress UDP/53 (DNS) and TCP/443 (DNS over HTTPS) to prevent client from using external DNS name server for resolution.;Enabling of Cloud DNS logging might result in your project being charged for the additional logs usage.;"**From Google Cloud CLI****Add New DNS Policy With Logging Enabled**For each VPC network that needs a DNS policy with logging enabled:```gcloud dns policies create enable-dns-logging --enable-logging --description=""Enable DNS Logging"" --networks=VPC_NETWORK_NAME```The VPC_NETWORK_NAME can be one or more networks in comma-separated list**Enable Logging for Existing DNS Policy**For each VPC network that has an existing DNS policy that needs logging enabled:```gcloud dns policies update POLICY_NAME --enable-logging --networks=VPC_NETWORK_NAME```The VPC_NETWORK_NAME can be one or more networks in comma-separated list";"**From Google Cloud CLI**1. List all VPCs networks in a project:```gcloud compute networks list --format=""table[box,title='All VPC Networks'](name:label='VPC Network Name')""```2. List all DNS policies, logging enablement, and associated VPC networks:```gcloud dns policies list --flatten=""networks[]"" --format=""table[box,title='All DNS Policies By VPC Network'](name:label='Policy Name',enableLogging:label='Logging Enabled':align=center,networks.networkUrl.basename():label='VPC Network Name')""```Each VPC Network should be associated with a DNS policy with logging enabled.";Additional Info- Only queries that reach a name server are logged. Cloud DNS resolvers cache responses, queries answered from caches, or direct queries to an external DNS resolver outside the VPC are not logged.;https://cloud.google.com/dns/docs/monitoring;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;2.14;Ensure 'Access Transparency' is 'Enabled';2 Logging and Monitoring;;Level 2;Manual;GCP Access Transparency provides audit logs for all actions that Google personnel take in your Google Cloud resources.;Controlling access to your information is one of the foundations of information security. Given that Google Employees do have access to your organizations' projects for support reasons, you should have logging in place to view who, when, and why your information is being accessed.;To use Access Transparency your organization will need to have at one of the following support level: Premium, Enterprise, Platinum, or Gold. There will be subscription costs associated with support, as well as increased storage costs for storing the logs. You will also not be able to turn Access Transparency off yourself, and you will need to submit a service request to Google Cloud Support.;**From Google Cloud Console****Add privileges to enable Access Transparency**1. From the Google Cloud Home, within the project you wish to check, click on the Navigation hamburger menu in the top left. Hover over the 'IAM and Admin'. Select `IAM` in the top of the column that opens. 2. Click the blue button the says `+add` at the top of the screen.3. In the `principals` field, select a user or group by typing in their associated email address.4. Click on the `role` field to expand it. In the filter field enter `Access Transparency Admin` and select it.5. Click `save`.**Verify that the Google Cloud project is associated with a billing account**1. From the Google Cloud Home, click on the Navigation hamburger menu in the top left. Select `Billing`.2. If you see `This project is not associated with a billing account` you will need to enter billing information or switch to a project with a billing account.**Enable Access Transparency**1. From the Google Cloud Home, click on the Navigation hamburger menu in the top left. Hover over the IAM & Admin Menu. Select `settings` in the middle of the column that opens.2. Click the blue button labeled Enable `Access Transparency for Organization`;**From Google Cloud Console****Determine if Access Transparency is Enabled**1. From the Google Cloud Home, click on the Navigation hamburger menu in the top left. Hover over the IAM & Admin Menu. Select `settings` in the middle of the column that opens.2. The status will be under the heading `Access Transparency`. Status should be `Enabled`;To enable Access Transparency for your Google Cloud organization, your Google Cloud organization must have one of the following customer support levels: Premium, Enterprise, Platinum, or Gold.;https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/overview:https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/enable:https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/reading-logs:https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/reading-logs#justification_reason_codes:https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/supported-services;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;3.9;Ensure No HTTPS or SSL Proxy Load Balancers Permit SSL Policies With Weak Cipher Suites;3 Networking;;Level 1;Manual;"Secure Sockets Layer (SSL) policies determine what port Transport Layer Security (TLS) features clients are permitted to use when connecting to load balancers. To prevent usage of insecure features, SSL policies should use (a) at least TLS 1.2 with the MODERN profile; or (b) the RESTRICTED profile, because it effectively requires clients to use TLS 1.2 regardless of the chosen minimum TLS version; or (3) a CUSTOM profile that does not support any of the following features: ```TLS_RSA_WITH_AES_128_GCM_SHA256TLS_RSA_WITH_AES_256_GCM_SHA384TLS_RSA_WITH_AES_128_CBC_SHATLS_RSA_WITH_AES_256_CBC_SHATLS_RSA_WITH_3DES_EDE_CBC_SHA```";Load balancers are used to efficiently distribute traffic across multiple servers. Both SSL proxy and HTTPS load balancers are external load balancers, meaning they distribute traffic from the Internet to a GCP network. GCP customers can configure load balancer SSL policies with a minimum TLS version (1.0, 1.1, or 1.2) that clients can use to establish a connection, along with a profile (Compatible, Modern, Restricted, or Custom) that specifies permissible cipher suites. To comply with users using outdated protocols, GCP load balancers can be configured to permit insecure cipher suites. In fact, the GCP default SSL policy uses a minimum TLS version of 1.0 and a Compatible profile, which allows the widest range of insecure cipher suites. As a result, it is easy for customers to configure a load balancer without even knowing that they are permitting outdated cipher suites.;Creating more secure SSL policies can prevent clients using older TLS versions from establishing a connection.;**From Google Cloud Console**If the TargetSSLProxy or TargetHttpsProxy does not have an SSL policy configured, create a new SSL policy. Otherwise, modify the existing insecure policy. 1. Navigate to the `SSL Policies` page by visiting: [https://console.cloud.google.com/net-security/sslpolicies](https://console.cloud.google.com/net-security/sslpolicies)2. Click on the name of the insecure policy to go to its `SSL policy details` page.3. Click `EDIT`.4. Set `Minimum TLS version` to `TLS 1.2`.5. Set `Profile` to `Modern` or `Restricted`. 6. Alternatively, if teh user selects the profile `Custom`, make sure that the following features are disabled: ```TLS_RSA_WITH_AES_128_GCM_SHA256TLS_RSA_WITH_AES_256_GCM_SHA384TLS_RSA_WITH_AES_128_CBC_SHATLS_RSA_WITH_AES_256_CBC_SHATLS_RSA_WITH_3DES_EDE_CBC_SHA```**From Google Cloud CLI**1. For each insecure SSL policy, update it to use secure cyphers:```gcloud compute ssl-policies update NAME [--profile COMPATIBLE|MODERN|RESTRICTED|CUSTOM] --min-tls-version 1.2 [--custom-features FEATURES]```2. If the target proxy has a GCP default SSL policy, use the following command corresponding to the proxy type to update it.```gcloud compute target-ssl-proxies update TARGET_SSL_PROXY_NAME --ssl-policy SSL_POLICY_NAMEgcloud compute target-https-proxies update TARGET_HTTPS_POLICY_NAME --ssl-policy SSL_POLICY_NAME```;**From Google Cloud Console**1. See all load balancers by visiting [https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list).2. For each load balancer for `SSL (Proxy)` or `HTTPS`, click on its name to go the `Load balancer details` page.3. Ensure that each target proxy entry in the `Frontend` table has an `SSL Policy` configured. 4. Click on each SSL policy to go to its `SSL policy details` page.5. Ensure that the SSL policy satisfies one of the following conditions: - has a `Min TLS` set to `TLS 1.2` and `Profile` set to `Modern` profile, or- has `Profile` set to `Restricted`. Note that a Restricted profile effectively requires clients to use TLS 1.2 regardless of the chosen minimum TLS version, or- has `Profile` set to `Custom` and the following features are all disabled:```TLS_RSA_WITH_AES_128_GCM_SHA256TLS_RSA_WITH_AES_256_GCM_SHA384TLS_RSA_WITH_AES_128_CBC_SHATLS_RSA_WITH_AES_256_CBC_SHATLS_RSA_WITH_3DES_EDE_CBC_SHA```**From Google Cloud CLI**1. List all TargetHttpsProxies and TargetSslProxies.```gcloud compute target-https-proxies listgcloud compute target-ssl-proxies list```2. For each target proxy, list its properties:```gcloud compute target-https-proxies describe TARGET_HTTPS_PROXY_NAMEgcloud compute target-ssl-proxies describe TARGET_SSL_PROXY_NAME```3. Ensure that the `sslPolicy` field is present and identifies the name of the SSL policy: ```sslPolicy: https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/sslPolicies/SSL_POLICY_NAME```If the `sslPolicy` field is missing from the configuration, it means that the GCP default policy is used, which is insecure.4. Describe the SSL policy:```gcloud compute ssl-policies describe SSL_POLICY_NAME```5. Ensure that the policy satisfies one of the following conditions:- has `Profile` set to `Modern` and `minTlsVersion` set to `TLS_1_2`, or- has `Profile` set to `Restricted`, or- has `Profile` set to `Custom` and  `enabledFeatures` does not contain any of the following values:```TLS_RSA_WITH_AES_128_GCM_SHA256TLS_RSA_WITH_AES_256_GCM_SHA384TLS_RSA_WITH_AES_128_CBC_SHATLS_RSA_WITH_AES_256_CBC_SHATLS_RSA_WITH_3DES_EDE_CBC_SHA```;;https://cloud.google.com/load-balancing/docs/use-ssl-policies:https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r2.pdf;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;3.10;Use Identity Aware Proxy (IAP) to Ensure Only Traffic From Google IP Addresses are 'Allowed';3 Networking;;Level 2;Manual;IAP authenticates the user requests to your apps via a Google single sign in. You can then manage these users with permissions to control access. It is recommended to use both IAP permissions and firewalls to restrict this access to your apps with sensitive information.;IAP ensure that access to VMs is controlled by authenticating incoming requests. Access to your apps and the VMs should be restricted by firewall rules that allow only the proxy IAP IP addresses contained in the 35.235.240.0/20 subnet. Otherwise, unauthenticated requests can be made to your apps. To ensure that load balancing works correctly health checks should also be allowed.;If firewall rules are not configured correctly, legitimate business services could be negatively impacted. It is recommended to make these changes during a time of low usage.;"**From Google Cloud Console**1. Go to the Cloud Console [VPC network > Firewall rules](https://console.cloud.google.com/networking/firewalls/list?_ga=2.72166934.480049361.1580860862-1336643914.1580248695).2. Select the checkbox next to the following rules: - default-allow-http - default-allow-https - default-allow-internal3. Click `Delete`.4. Click `Create firewall rule` and set the following values: - Name: allow-iap-traffic - Targets: All instances in the network - Source IP ranges (press Enter after you paste each value in the box, copy each full CIDR IP address): - IAP Proxy Addresses `35.235.240.0/20` - Google Health Check `130.211.0.0/22` - Google Health Check `35.191.0.0/16` - Protocols and ports: - Specified protocols and ports required for access and management of your app. For example most health check connection protocols would be covered by; - tcp:80 (Default HTTP Health Check port) - tcp:443 (Default HTTPS Health Check port)**Note: if you have custom ports used by your load balancers, you will need to list them here**5. When you're finished updating values, click `Create`.";"**From Google Cloud Console**1. For each of your apps that have IAP enabled go to the Cloud Console VPC network > Firewall rules.2. Verify that the only rules correspond to the following values: - Targets: All instances in the network - Source IP ranges: - IAP Proxy Addresses `35.235.240.0/20` - Google Health Check `130.211.0.0/22` - Google Health Check `35.191.0.0/16` - Protocols and ports: - Specified protocols and ports required for access and management of your app. For example most health check connection protocols would be covered by; - tcp:80 (Default HTTP Health Check port) - tcp:443 (Default HTTPS Health Check port)**Note: if you have custom ports used by your load balancers, you will need to list them here**";;https://cloud.google.com/iap/docs/concepts-overview:https://cloud.google.com/iap/docs/load-balancer-howto:https://cloud.google.com/load-balancing/docs/health-checks:https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-bastion-hosts;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;4.10;Ensure That App Engine Applications Enforce HTTPS Connections;4 Virtual Machines;;Level 2;Manual;In order to maintain the highest level of security all connections to an application should be secure by default.;Insecure HTTP connections maybe subject to eavesdropping which can expose sensitive data.;All connections to appengine will automatically be redirected to the HTTPS endpoint ensuring that all connections are secured by TLS.;Add a line to the app.yaml file controlling the application which enforces secure connections. For example```handlers:- url: /.* **secure: always** redirect_http_response_code: 301 script: auto```[https://cloud.google.com/appengine/docs/standard/python3/config/appref];Verify that the app.yaml file controlling the application contains a line which enforces secure connections. For example```handlers:- url: /.* secure: always redirect_http_response_code: 301 script: auto```[https://cloud.google.com/appengine/docs/standard/python3/config/appref](https://cloud.google.com/appengine/docs/standard/python3/config/appref);;https://cloud.google.com/appengine/docs/standard/python3/config/appref:https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;4.12;Ensure the Latest Operating System Updates Are Installed On Your Virtual Machines in All Projects;4 Virtual Machines;;Level 2;Manual;Google Cloud Virtual Machines have the ability via an OS Config agent API to periodically (about every 10 minutes) report OS inventory data. A patch compliance API periodically reads this data, and cross references metadata to determine if the latest updates are installed.This is not the only Patch Management solution available to your organization and you should weigh your needs before committing to using this method.;Keeping virtual machine operating systems up to date is a security best practice. Using this service will simplify this process.;Most Operating Systems require a restart or changing critical resources to apply the updates. Using the Google Cloud VM manager for its OS Patch management will incur additional costs for each VM managed by it. Please view the VM manager pricing reference for further information.;"**From Google Cloud Console****Enabling OS Patch Management on a Project by Project Basis****Install OS Config API for the Project**1. Navigate into a project. In the expanded portal menu located at the top left of the screen hover over ""APIs & Services"". Then in the menu right of that select ""API Libraries""2. Search for ""VM Manager (OS Config API) or scroll down in the left hand column and select the filter labeled ""Compute"" where it is the last listed. Open this API.3. Click the blue 'Enable' button.**Add MetaData Tags for OSConfig Parsing**1. From the main Google Cloud console, open the portal menu in the top left. Mouse over Computer Engine to expand the menu next to it.2. Under the ""Settings"" heading, select ""Metadata"".3. In this view there will be a list of the project wide metadata tags for VMs. Click edit and 'add item' in the key column type 'enable-osconfig' and in the value column set it to 'true'.From Command Line1. For project wide tagging, run the following command```gcloud compute project-info add-metadata \ --project <PROJECT_ID>\ --metadata=enable-osconfig=TRUE```Please see the reference /compute/docs/troubleshooting/vm-manager/verify-setup#metadata-enabled at the bottom for more options like instance specific tagging.Note: Adding a new tag via commandline may overwrite existing tags. You will need to do this at a time of low usage for the least impact.**Install and Start the Local OSConfig for Data Parsing**There is no way to centrally manage or start the Local OSConfig agent. Please view the reference of manage-os#agent-install to view specific operating system commands. **Setup a project wide Service Account**Please view Recommendation 4.1 to view how to setup a service account. Rerun the audit procedure to test if it has taken effect.**Enable NAT or Configure Private Google Access to allow Access to Public Update Hosting**For the sake of brevity, please see the attached resources to enable NAT or Private Google Access. Rerun the audit procedure to test if it has taken effect.From Command Line:**Install OS Config API for the Project**1. In each project you wish to audit run ```gcloud services enable osconfig.googleapis.com```**Install and Start the Local OSConfig for Data Parsing**Please view the reference of manage-os#agent-install to view specific operating system commands.**Setup a project wide Service Account**Please view Recommendation 4.1 to view how to setup a service account. Rerun the audit procedure to test if it has taken effect.**Enable NAT or Configure Private Google Access to allow Access to Public Update Hosting**For the sake of brevity, please see the attached resources to enable NAT or Private Google Access. Rerun the audit procedure to test if it has taken effect.Determine if Instances can connect to public update hostingLinux Debian Based Operating Systems```sudo apt update```The output should have a numbered list of lines with Hit: URL of updates.Redhat Based Operating Systems```yum check-update```The output should show a list of packages that have updates available.Windows```ping http://windowsupdate.microsoft.com/```The ping should successfully be delivered and received.";"**From Google Cloud Console****Determine if OS Config API is Enabled for the Project**1. Navigate into a project. In the expanded navigation menu located at the top left of the screen hover over `APIs & Services`. Then in the menu right of that select `API Libraries`2. Search for ""VM Manager (OS Config API) or scroll down in the left hand column and select the filter labeled ""Compute"" where it is the last listed. Open this API.3. Verify the blue button at the top is enabled.**Determine if VM Instances have correct metadata tags for OSConfig parsing**1. From the main Google Cloud console, open the hamburger menu in the top left. Mouse over Computer Engine to expand the menu next to it.1. Under the ""Settings"" heading, select ""Metadata"".1. In this view there will be a list of the project wide metadata tags for VMs. Determine if the tag ""enable-osconfig"" is set to ""true"".**Determine if the Operating System of VM Instances have the local OS-Config Agent running**There is no way to determine this from the Google Cloud console. The only way is to run operating specific commands locally inside the operating system via remote connection. For the sake of brevity of this recommendation please view the docs/troubleshooting/vm-manager/verify-setup reference at the bottom of the page. If you initialized your VM instance with a Google Supplied OS Image with a build date of later than v20200114 it will have the service installed. You should still determine its status for proper operation.**Verify the service account you have setup for the project in Recommendation 4.1 is running**1. Go to the `VM instances` page by visiting: [https://console.cloud.google.com/compute/instances](https://console.cloud.google.com/compute/instances).2. Click on each instance name to go to its `VM instance details` page.3. Under the section `Service Account`, take note of the service account4. Run the commands locally for your operating system that are located at the docs/troubleshooting/vm-manager/verify-setup#service-account-enabled reference located at the bottom of this page. They should return the name of your service account.**Determine if Instances can connect to public update hosting**Each type of operating system has its own update process. You will need to determine on each operating system that it can reach the update servers via its network connection. The VM Manager doesn't host the updates, it will only allow you to centrally issue a command to each VM to update.**Determine if OS Config API is Enabled for the Project**1. In each project you wish to enable run the following command ```gcloud services list```2. If osconfig.googleapis.com is in the left hand column it is enabled for this project.**Determine if VM Manager is Enabled for the Project**1. Within the project run the following command:```gcloud compute instances os-inventory describe VM-NAME \ --zone=ZONE```The output will look like```INSTANCE_ID INSTANCE_NAME OS OSCONFIG_AGENT_VERSION UPDATE_TIME29255009728795105 centos7 CentOS Linux 7 (Core) 20210217.00-g1.el7 2021-04-12T22:19:36.559Z5138980234596718741 rhel-8 Red Hat Enterprise Linux 8.3 (Ootpa) 20210316.00-g1.el8 2021-09-16T17:19:24Z7127836223366142250 windows Microsoft Windows Server 2019 Datacenter 20210316.00.0+win@1 2021-09-16T17:13:18Z```**Determine if VM Instances have correct metadata tags for OSConfig parsing**1. Select the project you want to view tagging in.**From Google Cloud Console**1. From the main Google Cloud console, open the hamburger menu in the top left. Mouse over Computer Engine to expand the menu next to it.2. Under the ""Settings"" heading, select ""Metadata"".3. In this view there will be a list of the project wide metadata tags for Vms. Verify a tag of ‘enable-osconfig’ is in this list and it is set to ‘true’.**From Command Line**Run the following command to view instance data```gcloud compute instances list --format=""table(name,status,tags.list())""```On each instance it should have a tag of ‘enable-osconfig’ set to ‘true’**Determine if the Operating System of VM Instances have the local OS-Config Agent running**There is no way to determine this from the Google Cloud CLI. The best way is to run the the commands inside the operating system located at 'Check OS-Config agent is installed and running' at the /docs/troubleshooting/vm-manager/verify-setup reference at the bottom of the page. If you initialized your VM instance with a Google Supplied OS Image with a build date of later than v20200114 it will have the service installed. You should still determine its status.**Verify the service account you have setup for the project in Recommendation 4.1 is running**1. Go to the `VM instances` page by visiting: [https://console.cloud.google.com/compute/instances](https://console.cloud.google.com/compute/instances).2. Click on each instance name to go to its `VM instance details` page.3. Under the section `Service Account`, take note of the service account4. View the compute/docs/troubleshooting/vm-manager/verify-setup#service-account-enabled resource at the bottom of the page for operating system specific commands to run locally.**Determine if Instances can connect to public update hosting**Linux Debian Based Operating Systems```sudo apt update```The output should have a numbered list of lines with Hit: URL of updates.Redhat Based Operating Systems```yum check-update```The output should show a list of packages that have updates available.Windows```ping http://windowsupdate.microsoft.com/```The ping should successfully be delivered and received.";This is not your only solution to handle updates. This is a Google Cloud specific recommendation to leverage a resource to solve the need for comprehensive update procedures and policy. If you have a solution already in place you do not need to make the switch.There are also further resources that would be out of the scope of this recommendation. If you need to allow your VMs to access public hosted updates, please see the reference to setup NAT or Private Google Access.;https://cloud.google.com/compute/docs/manage-os:https://cloud.google.com/compute/docs/os-patch-management:https://cloud.google.com/compute/docs/vm-manager:https://cloud.google.com/compute/docs/images/os-details#vm-manager:https://cloud.google.com/compute/docs/vm-manager#pricing:https://cloud.google.com/compute/docs/troubleshooting/vm-manager/verify-setup:https://cloud.google.com/compute/docs/instances/view-os-details#view-data-tools:https://cloud.google.com/compute/docs/os-patch-management/create-patch-job:https://cloud.google.com/nat/docs/set-up-network-address-translation:https://cloud.google.com/vpc/docs/configure-private-google-access:https://workbench.cisecurity.org/sections/811638/recommendations/1334335:https://cloud.google.com/compute/docs/manage-os#agent-install:https://cloud.google.com/compute/docs/troubleshooting/vm-manager/verify-setup#service-account-enabled:https://cloud.google.com/compute/docs/os-patch-management#use-dashboard:https://cloud.google.com/compute/docs/troubleshooting/vm-manager/verify-setup#metadata-enabled;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;6.1.1;Ensure That a MySQL Database Instance Does Not Allow Anyone To Connect With Administrative Privileges;6 Cloud SQL Database Services;6.1 MySQL Database;Level 1;Manual;It is recommended to set a password for the administrative user (`root` by default) to prevent unauthorized access to the SQL database instances.This recommendation is applicable only for MySQL Instances. PostgreSQL does not offer any setting for No Password from the cloud console.;At the time of MySQL Instance creation, not providing an administrative password allows anyone to connect to the SQL database instance with administrative privileges. The root password should be set to ensure only authorized users have these privileges.;Connection strings for administrative clients need to be reconfigured to use a password.;**From Google Cloud Console**1. Go to the Cloud SQL Instances page in the Google Cloud Platform Console using `https://console.cloud.google.com/sql/`2. Select the instance to open its Overview page.3. Select `Access Control > Users`.4. Click the `More actions icon` for the user to be updated.5. Select `Change password`, specify a `New password`, and click `OK`.**From Google Cloud CLI**1. Set a password to a MySql instance:```gcloud sql users set-password root --host=<host> --instance=<instance_name> --prompt-for-password```2. A prompt will appear, requiring the user to enter a password:```Instance Password:```3. With a successful password configured, the following message should be seen:```Updating Cloud SQL user...done.```;"**From Google Cloud CLI**1. List All SQL database instances of type MySQL:```gcloud sql instances list --filter='DATABASE_VERSION:MYSQL* --project <project_id> --format=""(NAME,PRIMARY_ADDRESS)""'```2. For every MySQL instance try to connect using the `PRIMARY_ADDRESS`, if available:```mysql -u root -h <mysql_instance_ip_address>```The command should return either an error message or a password prompt.Sample Error message:```ERROR 1045 (28000): Access denied for user 'root'@'<Instance_IP>' (using password: NO)```If a command produces the `mysql>` prompt, the MySQL instance allows anyone to connect with administrative privileges without needing a password.**Note:** The `No Password` setting is exposed only at the time of MySQL instance creation. Once the instance is created, the Google Cloud Platform Console does not expose the set to confirm whether a password for an administrative user is set to a MySQL instance.";;https://cloud.google.com/sql/docs/mysql/create-manage-users:https://cloud.google.com/sql/docs/mysql/create-instance;MANUAL;Manual check;manual_check;Manual check;manual;False
gcp;The CIS Google Cloud Platform Foundations Benchmark provides prescriptive guidance for configuring security options for a subset of GCP with an emphasis on foundational, testable, and architecture agnostic settings.;;;2025-07-01 12:41:26.937059;7.4;Ensure all data in BigQuery has been classified;7 BigQuery;;Level 2;Manual;BigQuery tables can contain sensitive data that for security purposes should be discovered, monitored, classified, and protected. Google Cloud's Sensitive Data Protection tools can automatically provide data classification of all BigQuery data across an organization.;Using a cloud service or 3rd party software to continuously monitor and automate the process of data discovery and classification for BigQuery tables is an important part of protecting the data.Sensitive Data Protection is a fully managed data protection and data privacy platform that uses machine learning and pattern matching to discover and classify sensitive data in Google Cloud.;There is a cost associated with using Sensitive Data Protection. There is also typically a cost associated with 3rd party tools that perform similar processes and protection.;"**Enable profiling:**1. Go to Cloud DLP by visiting https://console.cloud.google.com/dlp/landing/dataProfiles/configurations1. Click ""Create Configuration""1. For projects follow https://cloud.google.com/dlp/docs/profile-project. For organizations or folders follow https://cloud.google.com/dlp/docs/profile-org-folder **Review findings:**- Columns or tables with high data risk have evidence of sensitive information without additional protections. To lower the data risk score, consider doing the following:- For columns containing sensitive data, apply a BigQuery policy tag to restrict access to accounts with specific access rights.- De-identify the raw sensitive data using de-identification techniques like masking and tokenization.**Incorporate findings into your security and governance operations:**- Enable sending findings into your security and posture services. You can publish data profiles to Security Command Center and Chronicle.- Automate remediation or enable alerting of new or changed data risk with Pub/Sub.";1. Go to Cloud DLP by visiting https://console.cloud.google.com/dlp/landing/dataProfiles/configurations.2. Verify there is a discovery scan configuration either for the organization or project.;;https://cloud.google.com/dlp/docs/data-profiles:https://cloud.google.com/dlp/docs/analyze-data-profiles:https://cloud.google.com/dlp/docs/data-profiles-remediation:https://cloud.google.com/dlp/docs/send-profiles-to-scc:https://cloud.google.com/dlp/docs/profile-org-folder#chronicle:https://cloud.google.com/dlp/docs/profile-org-folder#publish-pubsub;MANUAL;Manual check;manual_check;Manual check;manual;False
